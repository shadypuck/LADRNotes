\documentclass[../main.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter: #1}{}}
\setcounter{chapter}{6}

\begin{document}




\chapter{Operators on Inner Product Spaces}
\section{Self-Adjoints and Normal Operators}
\begin{itemize}
    \item \marginnote{10/7:}\textbf{Adjoint} (of $T\in\lin{V}{W}$): The function $T^*:W\to V$ that satisfies
    \begin{equation*}
        \inp{Tv}{w} = \inp{v}{T^*w}
    \end{equation*}
    for all $v\in V$ and $w\in W$\footnote{Note that the word adjoint has another, unrelated meaning in algebra. Fortunately, this other meaning will not be covered in \textcite{bib:Axler}.}.
    \begin{itemize}
        \item Calculating $T^*w$: Consider the linear functional $\varphi:V\to\F$ defined by $\varphi(v)=\inp{Tv}{w}$ for all $v\in V$. By the \hyperref[trm:RieszRepresentationTheorem]{Riesz Representation Theorem}, there exists a unique vector $T^*w\in V$ such that $\varphi(v)=\inp{v}{T^*w}$ for all $v\in V$. This vector in $V$ will guarantee that $\inp{Tv}{w}=\varphi(v)=\inp{v}{T^*w}$ for all $v\in V$, and we can find vectors $T^*w\in V$ for all $w\in W$.
    \end{itemize}
    \item The adjoint is a linear map.
    \begin{theorem}
        If $T\in\lin{V}{W}$, then $T^*\in\lin{W}{V}$.
        \begin{proof}
            Let $T\in\lin{V}{W}$, let $w_1,w_2\in W$, and let $\lambda\in\F$. By the definition of $T^*$, we have that for any $v\in V$,
            \begin{align*}
                \inp{v}{T^*(w_1+w_2)} &= \inp{Tv}{w_1+w_2}&
                    \inp{v}{T^*(\lambda w_1)} &= \inp{Tv}{\lambda w_1}\\
                &= \inp{Tv}{w_1}+\inp{Tv}{w_2}&
                    &= \bar{\lambda}\inp{Tv}{w_1}\\
                &= \inp{v}{T^*w_1}+\inp{v}{T^*w_2}&
                    &= \bar{\lambda}\inp{v}{T^*w_1}\\
                &= \inp{v}{T^*w_1+T^*w_2}&
                    &= \inp{v}{\lambda T^*w_1}
            \end{align*}
            Thus, by the definition of $T^*$,
            \begin{align*}
                T^*(w_1+w_2) &= T^*w_1+T^*w_2&
                T^*(\lambda w_1) &= \lambda T^*w
            \end{align*}
            so $T^*$ is a linear map, as desired.
        \end{proof}
    \end{theorem}
    \item Properties of the adjoint.
    \begin{theorem}\label{trm:adjointProperties}\leavevmode
        \begin{enumerate}[label={\textup{(}\alph*\textup{)}},ref={\thetheorem\alph*}]
            \item \label{trm:adjointPropertiesa}$(S+T)^*=S^*+T^*$ for all $S<T\in\lin{V}{W}$.
            \begin{proof}
                Suppose $S,T\in\lin{V}{W}$. If $v\in V$ and $w\in W$, then
                \begin{align*}
                    \inp{v}{(S+T)^*w} &= \inp{(S+T)v}{w}\\
                    &= \inp{Sv}{w}+\inp{Tv}{w}\\
                    &= \inp{v}{S^*w}+\inp{v}{T^*w}\\
                    &= \inp{v}{S^*w+T^*w}
                \end{align*}
                Thus, $(S+T)^*w=S^*w+T^*w$, as desired.
            \end{proof}
            \item \label{trm:adjointPropertiesb}$(\lambda T)^*=\bar{\lambda}T^*$ for all $\lambda\in\F$ and $T\in\lin{V}{W}$.
            \begin{proof}
                Suppose $T\in\lin{V}{W}$ and $\lambda\in\F$. If $v\in V$ and $w\in W$, then
                \begin{align*}
                    \inp{v}{(\lambda T)^*w} &= \inp{\lambda Tv}{w}\\
                    &= \lambda\inp{Tv}{w}\\
                    &= \lambda\inp{v}{T^*w}\\
                    &= \inp{v}{\bar{\lambda}T^*w}
                \end{align*}
                Thus, $(\lambda T)^*w=\bar{\lambda}T^*w$, as desired.
            \end{proof}
            \item \label{trm:adjointPropertiesc}$(T^*)^*=T$ for all $T\in\lin{V}{W}$.
            \begin{proof}
                Suppose $T\in\lin{V}{W}$. If $v\in V$ and $w\in W$, then
                \begin{align*}
                    \inp{w}{(T^*)^*v} &= \inp{T^*w}{v}\\
                    &= \overline{\inp{v}{T^*w}}\\
                    &= \overline{\inp{Tv}{w}}\\
                    &= \inp{w}{Tv}
                \end{align*}
                Thus, $(T^*)^*v=Tv$, as desired.
            \end{proof}
            \item \label{trm:adjointPropertiesd}$I^*=I$, where $I$ is the identity operator on $V$.
            \begin{proof}
                If $v,u\in V$, then
                \begin{equation*}
                    \inp{v}{I^*u} = \inp{Iv}{u} = \inp{v}{Iu}
                \end{equation*}
                Thus, $I^*u=Iu$, as desired.
            \end{proof}
            \item \label{trm:adjointPropertiese}$(ST)^*=T^*S^*$ for all $T\in\lin{V}{W}$ and $S\in\lin{W}{U}$. Here $U$ is an inner product space over $\F$.
            \begin{proof}
                Suppose $T\in\lin{V}{W}$ and $S\in\lin{W}{U}$. If $v\in V$ and $u\in U$, then
                \begin{align*}
                    \inp{v}{(ST)^*u} &= \inp{STv}{u}\\
                    &= \inp{Tv}{S^*u}\\
                    &= \inp{v}{T^*S^*u}
                \end{align*}
                Thus, $(ST)^*u=T^*S^*u$, as desired.
            \end{proof}
        \end{enumerate}
    \end{theorem}
    \item Null space and range of $T^*$.
    \begin{theorem}\label{trm:adjointNullRange}
        Suppose $T\in\lin{V}{W}$. Then
        \begin{enumerate}[label={\textup{(}\alph*\textup{)}},ref={\thetheorem\alph*}]
            \item \label{trm:adjointNullRangea}$\nul T^*=(\range T)^\perp$.
            \begin{proof}
                Let $w\in W$ be an arbitrary element of $\nul T^*$. Then $T^*w=0$ by definition. It follows by Theorem \ref{trm:inpPropertiesc} that $\inp{v}{T^*w}=0$ for all $v\in V$. Thus, by the definition of the adjoint, $\inp{Tv}{w}=0$ for all $v\in V$. But this implies that $w$ is orthogonal to every vector in $\range T$ (i.e., the set of all $Tv$), meaning that $w\in(\range T)^\perp$.\par
                The proof is symmetric in the other direction.
            \end{proof}
            \item \label{trm:adjointNullRangeb}$\range T^*=(\nul T)^\perp$.
            \begin{proof}
                We have that
                \begin{align*}
                    \range T^* &= ((\range T^*)^\perp)^\perp\tag*{Theorem \ref{trm:perpPerp}}\\
                    &= (\nul(T^*)^*)^\perp\tag*{Theorem \ref{trm:adjointNullRangea}}\\
                    &= (\nul T)^\perp\tag*{Theorem \ref{trm:adjointPropertiesc}}
                \end{align*}
                as desired.
            \end{proof}
            \item \label{trm:adjointNullRangec}$\nul T=(\range T^*)^\perp$.
            \begin{proof}
                We have that
                \begin{align*}
                    \nul T &= \nul(T^*)^*\tag*{Theorem \ref{trm:adjointPropertiesc}}\\
                    &= (\range T^*)^\perp\tag*{Theorem \ref{trm:adjointNullRangea}}
                \end{align*}
                as desired.
            \end{proof}
            \item \label{trm:adjointNullRanged}$\range T=(\nul T^*)^\perp$.
            \begin{proof}
                We have that
                \begin{align*}
                    \range T &= ((\range T)^\perp)^\perp\tag*{Theorem \ref{trm:perpPerp}}\\
                    &= (\nul T^*)^\perp\tag*{Theorem \ref{trm:adjointNullRangea}}
                \end{align*}
                as desired.
            \end{proof}
        \end{enumerate}
    \end{theorem}
    \item \textbf{Conjugate transpose} (of an $m$-by-$n$ matrix): The $n$-by-$m$ matrix obtained by interchanging the rows and columns and then taking the complex conjugate of each entry.
    \begin{itemize}
        \item "If $\F=\R$, then the conjugate transpose of a matrix is the same as its transpose" \parencite[207]{bib:Axler}.
    \end{itemize}
    \item The next result shows how to compute the matrix of $T^*$ from the matrix of $T$. Note, however, that if $\mat{T}$ is with respect to nonorthonormal bases, $\mat{T^*}$ does not necessarily equal the conjugate transpose of $\mat{T}$.
    \begin{theorem}
        Let $T\in\lin{V}{W}$. Suppose $e_1,\dots,e_n$ is an orthonormal basis of $V$ and $f_1,\dots,f_m$ is an orthonormal basis of $W$. Then
        \begin{equation*}
            \mat{T^*,(f_1,\dots,f_m),(e_1,\dots,e_n)}
        \end{equation*}
        is the conjugate transpose of
        \begin{equation*}
            \mat{T,(e_1,\dots,e_n),(f_1,\dots,f_m)}
        \end{equation*}
        \begin{proof}
            Recall that the $k^\text{th}$ column of $\mat{T}$ is given by writing $Te_k$ as a linear combination of the $f_j$'s. Since $f_1,\dots,f_m$ is an orthonormal basis of $W$, Theorem \ref{trm:linCombOrthonormal} implies that
            \begin{equation*}
                Te_k = \inp{Te_k}{f_1}f_1+\cdots+\inp{Te_k}{f_m}f_m
            \end{equation*}
            Thus, the entry in row $j$ column $k$ of $\mat{T}$ is $\inp{Te_k}{f_j}$. On the other hand, since
            \begin{equation*}
                T^*f_k = \inp{T^*f_k}{e_1}e_1+\cdots+\inp{T^*f_k}{e_n}e_n
            \end{equation*}
            we have that the entry in row $j$ column $k$ of $\mat{T^*}$ is
            \begin{align*}
                \inp{T^*f_k}{e_j} &= \inp{f_k}{Te_j}\\
                &= \overline{\inp{Te_j}{f_k}}
            \end{align*}
            Therefore, the entry in row $k$ column $j$ of $\mat{T^*}$ is the complex conjugate of the entry in row $j$ column $k$ of $\mat{T}$, as desired.
        \end{proof}
    \end{theorem}
    \item \textbf{Self-adjoint} (operator $T\in\ope{V}$): An operator $T$ such that $T=T^*$. \emph{Also known as} \textbf{Hermitian}.
    \begin{itemize}
        \item In other words, $T\in\ope{V}$ is self-adjoint if and only if
        \begin{equation*}
            \inp{Tv}{w} = \inp{v}{Tw}
        \end{equation*}
        for all $v,w\in V$.
    \end{itemize}
    \item The sum of two self-adjoint operators is self-adjoint, and the product of a real scalar and a self-adjoint operator is self-adjoint.
    \item Note the analogy between self-adjoint operator and complex numbers: A complex number $z$ is real iff $z=\bar{z}$, and thus a self-adjoint operator ($T=T^*$) is analogous to a real number.
    \item Eigenvalues of self-adjoint operators.
    \begin{theorem}
        Every eigenvalue of a self-adjoint operator is real.
        \begin{proof}
            Let $T$ be a self-adjoint operator on $V$, let $\lambda$ be an eigenvalue of $T$, and let $v$ be a nonzero vector in $V$ such that $Tv=\lambda v$. Then
            \begin{equation*}
                \lambda\norm{v}^2 = \inp{\lambda v}{v} = \inp{Tv}{v} = \inp{v}{Tv} = \inp{v}{\lambda v} = \bar{\lambda}\norm{v}^2
            \end{equation*}
            so $\lambda=\bar{\lambda}$, which implies that $\lambda$ is real, as desired.
        \end{proof}
    \end{theorem}
    \item The next result is false for real inner product spaces (consider a rotation matrix), but true for complex ones.
    \begin{theorem}\label{trm:inpTvvZero}
        Suppose $V$ is a complex inner product space and $T\in\ope{V}$. Suppose $\inp{Tv}{v}=0$ for all $v\in V$. Then $T=0$.
        \begin{proof}
            Let $u\in V$ be arbitrary. By inner product algebra, we have that
            \begin{equation*}
                \inp{Tu}{w} = \frac{\inp{T(u+w)}{u+w}-\inp{T(u-w)}{u-w}}{4}+\frac{\inp{T(u+iw)}{u+iw}-\inp{T(u-iw)}{u-iw}}{4}i
            \end{equation*}
            for all $w\in V$. Since each term on the right-hand side of the above equation is of the form $\inp{Tv}{v}$ and we know by hypothesis that $\inp{Tv}{v}=0$ for all $v\in V$, we have that $\inp{Tu}{w}=0$ for all $w\in V$. In particular, if we let $w=Tu$, we learn that $\inp{Tu}{Tu}=0$, which implies that $Tu=0$. But this implies that $Tu=0$ for all $u\in V$, i.e., that $T=0$.
        \end{proof}
    \end{theorem}
    \item The next result provides another example of how self-adjoint operators behave like real numbers, and is also false for real inner product spaces (consider a operator on such a space that is not self-adjoint).
    \begin{theorem}\label{trm:selfAdjointTvvReal}
        Suppose $V$ is a complex inner product space and $T\in\ope{V}$. Then $T$ is self-adjoint if and only if $\inp{Tv}{v}\in\R$ for every $v\in V$.
        \begin{proof}
            Suppose first that $T$ is self-adjoint. Let $v\in V$ be arbitrary. Then
            \begin{equation*}
                \inp{Tv}{v}-\overline{\inp{Tv}{v}} = \inp{Tv}{v}-\inp{v}{Tv}
                = \inp{Tv}{v}-\inp{T^*v}{v}
                = \inp{(T-T^*)v}{v}
                = \inp{0v}{v}
                = 0
            \end{equation*}
            so $\inp{Tv}{v}=\overline{\inp{Tv}{v}}$. Therefore, $\inp{Tv}{v}\in\R$, as desired.\par
            Now suppose that $\inp{Tv}{v}\in\R$ for every $v\in V$. Let $v\in V$ be arbitrary. Then
            \begin{equation*}
                \inp{(T-T^*)v}{v} = \inp{Tv}{v}-\inp{T^*v}{v}
                = \inp{Tv}{v}-\inp{v}{Tv}
                = \inp{Tv}{v}-\overline{\inp{Tv}{v}}
                = 0
            \end{equation*}
            Therefore, by Theorem \ref{trm:inpTvvZero}, $T-T^*=0$, or $T=T^*$, as desired.
        \end{proof}
    \end{theorem}
    \item We now show that on complex \emph{or} real vector spaces, self-adjoint operators that satisfy $\inp{Tv}{v}=0$ \emph{must} be the zero operator.
    \begin{theorem}\label{trm:selfAdjointTvv}
        Suppose $T$ is a self-adjoint operator on $V$ such that
        \begin{equation*}
            \inp{Tv}{v} = 0
        \end{equation*}
        for all $v\in V$. Then $T=0$.
        \begin{proof}
            We divide into two cases. If $V$ is complex, invoke Theorem \ref{trm:inpTvvZero}. If $V$ is real, we continue.\par
            Let $u\in V$ be arbitrary. By inner product algebra, we have that
            \begin{equation*}
                \inp{Tu}{w} = \frac{\inp{T(u+w)}{u+w}-\inp{T(u-w)}{u-w}}{4}
            \end{equation*}
            By a symmetric argument to that used in the later part of the proof of Theorem \ref{trm:inpTvvZero}, we can confirm that $T=0$.
        \end{proof}
    \end{theorem}
    \item \textbf{Normal} (operator): An operator that commutes with its adjoint.
    \begin{itemize}
        \item In other words, $T\in\ope{V}$ is normal if
        \begin{equation*}
            TT^* = T^*T
        \end{equation*}
    \end{itemize}
    \item Every self-adjoint operator is normal.
    \item We now characterize normal operators.
    \begin{theorem}\label{trm:normalNorm}
        An operator is normal if and only if
        \begin{equation*}
            \norm{Tv} = \norm{T^*v}
        \end{equation*}
        for all $v\in V$.
        \begin{proof}
            Let $T\in\ope{V}$.\par
            Suppose first that $T$ is normal. Then $T^*T-TT^*=0$. Thus, by Theorem \ref{trm:inpPropertiesb}, $\inp{(T^*T-TT^*)v}{v}=0$ for all $v\in V$. It follows that
            \begin{align*}
                \inp{T^*Tv}{v} &= \inp{TT^*v}{v}\\
                \inp{Tv}{Tv} &= \inp{T^*v}{T^*v}\\
                \norm{Tv}^2 &= \norm{T^*v}^2\\
                \norm{Tv} &= \norm{T^*v}
            \end{align*}
            for all $v\in V$, as desired.\par
            Now suppose that $\norm{Tv}=\norm{T^*v}$ for all $v\in V$. Then following the reverse of the procedure for the forward direction, we can easily show that $\inp{(T^*T-TT^*)v}{v}=0$ for all $v\in V$. Additionally, by consecutive applications of Theorem \ref{trm:adjointProperties}, we have that
            \begin{align*}
                (T^*T-TT^*)^* &= (T^*T)^*-(TT^*)^*\\
                &= T^*(T^*)^*-(T^*)^*T^*\\
                &= T^*T-TT^*
            \end{align*}
            It follows that $T^*T-TT^*$ is self-adjoint. This combined with the previous result implies by Theorem \ref{trm:selfAdjointTvv} that $T^*T-TT^*=0$. It follows that $T^*T=TT^*$, so $T$ is normal, as desired.
        \end{proof}
    \end{theorem}
    \item While an operator and its adjoint may have different eigenvectors, a normal operator and its adjoint have the same eigenvectors.
    \begin{theorem}\label{trm:normalSameEigenvectors}
        Suppose $T\in\ope{V}$ is normal and $v\in V$ is an eigenvector of $T$ with eigenvalue $\lambda$. Then $v$ is also an eigenvector of $T^*$ with eigenvalue $\bar{\lambda}$.
        \begin{proof}
            By consecutive applications of Theorem \ref{trm:adjointProperties}, we have that
            \begin{align*}
                (T-\lambda I)(T-\lambda I)^* &= (T-\lambda I)(T^*-\bar{\lambda}I)\\
                &= TT^*-\bar{\lambda}T-\lambda T^*+\lambda\bar{\lambda}I\\
                &= T^*T-\lambda T^*-\bar{\lambda}T+\bar{\lambda}\lambda I\\
                &= (T^*-\bar{\lambda}I)(T-\lambda I)\\
                &= (T-\lambda I)^*(T-\lambda I)
            \end{align*}
            Thus, $T-\lambda I$ is self-adjoint. It follows by Theorem \ref{trm:normalNorm} that
            \begin{equation*}
                0 = \norm{(T-\lambda I)v} = \norm{(T-\lambda I)^*v} = \norm{(T^*-\bar{\lambda}I)v}
            \end{equation*}
            Hence $(T^*-\bar{\lambda}I)v=0$, so $T^*v=\bar{\lambda}v$, so $v$ is an eigenvector of $T^*$ with eigenvalue $\bar{\lambda}$, as desired.
        \end{proof}
    \end{theorem}
    \item Normal operators have orthogonal eigenvectors.
    \begin{theorem}
        Suppose $T\in\ope{V}$ is normal. Then the eigenvectors of $T$ corresponding to distinct eigenvalues are orthogonal.
        \begin{proof}
            Let $\alpha,\beta$ be distinct eigenvalues of $T$, and let $u,v$ be their corresponding eigenvectors. Thus, we have that
            \begin{align*}
                (\alpha-\beta)\inp{u}{v} &= \inp{\alpha u}{v}-\inp{u}{\bar{\beta}v}\\
                &= \inp{Tu}{v}-\inp{u}{T^*v}\tag*{Theorem \ref{trm:normalSameEigenvectors}}\\
                &= 0
            \end{align*}
            Since $\alpha\neq\beta$ by hypothesis, we must have that $\inp{u}{v}=0$. Therefore, $u,v$ are orthogonal, as desired.
        \end{proof}
    \end{theorem}
\end{itemize}



\section{The Spectral Theorem}
\begin{itemize}
    \item Diagonal operators are nice operators.
    \begin{itemize}
        \item An operator has a diagonal matrix with respect to some basis iff the basis consists of eigenvectors of the operator (see Theorem \ref{trm:diagonalizableConditions}).
    \end{itemize}
    \item The nicest operators are those for which there is an orthonormal basis of $V$ with respect to which the operator has a diagonal matrix.
    \begin{itemize}
        \item The Spectral Theorem characterizes the operators $T\in\ope{V}$ for which there exists an orthonormal basis of $V$ consisting of eigenvectors of $T$.
        \item In particular, it characterizes them as the normal operators when $\F=\C$ and the self-adjoint operators when $\F=\R$.
        \item "The Spectral Theorem is probably the most useful tool in the study of operators on inner product spaces" \parencite[217]{bib:Axler}.
    \end{itemize}
    \item For the purposes of proving the Spectral Theorem, we will break it into a Complex Spectral Theorem and a Real Spectral Theorem.
    \item The complex portion is simpler, so we begin with it.
    \begin{theorem}[Complex Spectral Theorem]\label{trm:ComplexSpectral}
        Suppoe $\F=\C$ and $T\in\ope{V}$. Then the following are equivalent.
        \begin{enumerate}[label={\textup{(}\alph*\textup{)}}]
            \item $T$ is normal.
            \item $V$ has an orthonormal basis consisting of eigenvectors of $T$.
            \item $T$ has a diagonal matrix with respect to some orthonormal basis of $V$.
        \end{enumerate}
        \begin{proof}
            We have by Theorem \ref{trm:diagonalizableConditions} that (b) and (c) are equivalent, so we will focus on proving the equivalence of (a) and (c).\par
            Suppose first that (c) holds. Since $\mat{T}$ is diagonal and $\mat{T^*}$ is the conjugate transpose of $\mat{T}$, $\mat{T^*}$ is diagonal. Therefore, since any two diagonal matrices commute, $T$ is normal, so (a) holds.\par
            Now suppose that (a) holds. By \hyperref[trm:Schur]{Schur's Theorem}, there exists an orthonormal basis $e_1,\dots,e_n$ of $V$ with respect to which $T$ has an upper triangular matrix. We will show that this matrix is actually diagonal. To begin, since $\mat{T}$ is upper triangular, we know that
            \begin{equation*}
                \norm{Te_1}^2 = |a_{1,1}|^2
            \end{equation*}
            Similarly, since $T^*$ is the conjugate \emph{transpose}, we have that
            \begin{equation*}
                \norm{T^*e_1}^2 = |a_{1,1}|^2+\cdots+|a_{1,n}|^2
            \end{equation*}
            But since $\norm{Te_1}=\norm{T^*e_1}$ by Theorem \ref{trm:normalNorm}, the two equations above imply that
            \begin{equation*}
                0 = |a_{1,2}|^2+\cdots+|a_{1,n}|^2
            \end{equation*}
            Therefore, we know that all entries in row 1 save the first are zero. We may repeat this procedure for every row to finish the proof.
        \end{proof}
    \end{theorem}
    \item The next result continues to build on the likeness of normal matrices and real numbers. Specifically, it plays off the fact that if $b,c\in\R$ with $b^2<4c$, then $x^2+bx+c>0$, i.e., $x^2+bx+c$ nonzero is an "invertible" real number.
    \begin{theorem}\label{trm:invertibleQuadratics}
        Suppose $T\in\ope{V}$ is self-adjoint and $b,c\in\R$ are such that $b^2<4c$. Then
        \begin{equation*}
            T^2+bT+cI
        \end{equation*}
        is invertible.
        \begin{proof}
            To prove that $T^2+bT+cI$ is invertible, Theorem \ref{trm:invertInjSurjFiniteEquivalence} tells us that it will suffice to show that $T$ is injective. To do this, Theorem \ref{trm:nullSpaceInjective} tells us that we must verify that $\nul(T^2+bT+cI)\subset\{0\}$, i.e., that if $v\in V$ is nonzero, then $(T^2+bT+cI)v\neq 0$. Let's begin.\par
            Let $v\in V$ be arbitrary. Then we have that
            \begin{align*}
                \inp{(T^2+bT+cI)v}{v} &= \inp{T^2v}{v}+b\inp{Tv}{v}+c\inp{v}{v}\\
                &= \inp{Tv}{Tv}+b\inp{Tv}{v}+c\norm{v}^2\\
                &\geq \norm{Tv}^2-|b|\norm{Tv}\norm{v}+c\norm{v}^2\tag*{\hyperref[trm:CauchySchwarz]{Cauchy-Schwarz Inequality}}\\
                &= \left( \norm{Tv}-\frac{|b|\norm{v}}{2} \right)^2+\left( c-\frac{b^2}{4} \right)\norm{v}^2\\
                &> 0
            \end{align*}
            The overall strict inequality implies by the contrapositive of Theorem \ref{trm:inpPropertiesb} that $(T^2+bT+cI)v\neq 0$, as desired.
        \end{proof}
    \end{theorem}
    \item Like Theorem \ref{trm:eigenExists} told us that operators on \emph{finite-dimensional nonzero complex} vector spaces have eigenvalues, the following tells us that \emph{self-adjoint} operators on \emph{any nonzero} vector space have eigenvalues.
    \begin{theorem}\label{trm:eigenSelfAdjoint}
        Suppose $V\neq\{0\}$ and $T\in\ope{V}$ is a self-adjoint operator. Then $T$ has an eigenvalue.
        \begin{proof}
            Let $V$ be a real inner product space, let $n=\dim V$, and let $v\in V$ be arbitrary and nonzero. Since $v,Tv,T^2v,\dots,T^nv$ has length $n+1>\dim V$, it is linearly dependent. Thus, there exist $a_0,\dots,a_n\in\F$ such that
            \begin{equation*}
                0 = a_0v+a_1Tv+\cdots+a_nT^nv
            \end{equation*}
            If we let the $a$'s be the coefficients of a degree $n$ polynomial, then we have by Theorem \ref{trm:realPolFactorization} that
            \begin{equation*}
                a_0+a_1x+\cdots+a_nx^n = c(x^2+b_1x+c_1)\cdots(x^2+b_Mx+c_M)(x-\lambda_1)\cdots(x-\lambda_m)
            \end{equation*}
            where $c\in\R$ is nonzero, each $b_j,c_j,\lambda_j\in\R$, each $b_j^2<4c_j$, $m+M\geq 1$, and the equation holds for all $x\in\R$. It follows that
            \begin{align*}
                0 &= a_0v+a_1Tv+\cdots+a_nT^nv\\
                &= (a_0I+a_1T+\cdots+a_nT^n)v\\
                &= c(T^2+b_1T+c_1I)\cdots(T_2+b_MT+c_MI)(T-\lambda_1I)\cdots(T-\lambda_mI)v
            \end{align*}
            Since $T$ is self-adjoint and $b_j,c_j\in\R$ satisfy $b_j^2<4c_j$ for each $j$, we have by consecutive applications of Theorem \ref{trm:invertibleQuadratics} that each $T^2+b_jT+c_jI$ is invertible. Thus, if we multiply both sides of the above equation by $1/c$ (recall that $c\neq 0$) and $(T^2+b_jT+c_jI)^{-1}$ for each $j$, we obtain
            \begin{equation*}
                0 = (T-\lambda I)\cdots(T-\lambda_mI)v
            \end{equation*}
            Therefore, by an argument symmetric to that used in the last paragraph of the proof of Theorem \ref{trm:eigenExists}, we have that $T$ has an eigenvalue, as desired.
        \end{proof}
    \end{theorem}
    \item Invariant subspaces and self-adjoint operators.
    \begin{theorem}\label{trm:invariantSelfAdjoint}
        Suppose $T\in\ope{V}$ is self-adjoint and $U$ is a subspace of $V$ that is invariant under $T$. Then
        \begin{enumerate}[label={\textup{(}\alph*\textup{)}},ref={\thetheorem\alph*}]
            \item \label{trm:invariantSelfAdjointa}$U^\perp$ is invariant under $T$.
            \begin{proof}
                Let $v\in U^\perp$ be arbitrary, and let $u$ be any element of $U$. Then
                \begin{equation*}
                    \inp{Tv}{u} = \inp{v}{Tu} = 0
                \end{equation*}
                where the first equality holds because $T$ is self-adjoint and the second equality holds because $U$ is invariant under $T$ (so $Tu\in U$, and we know that the inner product of an element of $U^\perp$ with an element of $U$ is 0). Thus, since $\inp{Tv}{u}=0$ for all $u\in U$, $Tv\in U^\perp$, as desired.
            \end{proof}
            \item \label{trm:invariantSelfAdjointb}$T|_U\in\ope{U}$ is self-adjoint.
            \begin{proof}
                If $u,v\in U$, then
                \begin{equation*}
                    \inp{(T|_U)u}{v} = \inp{Tu}{v} = \inp{u}{Tv} = \inp{u}{(T|_U)v}
                \end{equation*}
                as desired.
            \end{proof}
            \item \label{trm:invariantSelfAdjointc}$T|_{U^\perp}\in\ope{U^\perp}$ is self-adjoint.
            \begin{proof}
                The proof is symmetric to that of Theorem \ref{trm:invariantSelfAdjointb}.
            \end{proof}
        \end{enumerate}
    \end{theorem}
    \item We can now prove the real portion of the spectral theorem.
    \begin{theorem}[Real Spectral Theorem]\label{trm:RealSpectral}
        Suppose $\F=\R$ and $T\in\ope{V}$. Then the following are equivalent.
        \begin{enumerate}[label={\textup{(}\alph*\textup{)}}]
            \item $T$ is self-adjoint.
            \item $V$ has an orthonormal basis consisting of eigenvectors of $T$.
            \item $T$ has a diagonal matrix with respect to some orthonormal basis of $V$.
        \end{enumerate}
        \begin{proof}
            We will prove that (a) implies (b), (b) implies (c), and (c) implies (a). Let's begin.\par
            First, suppose that $T$ is self-adjoint. We induct on $\dim V$. For the base case $\dim V=1$, we must have $Tv=\lambda v$ for any $v\in V$. Thus, take $e=v/\norm{v}$ as an orthonormal basis of $V$ consisting of eigenvectors of $T$. Now suppose inductively that (a) implies (b) for all real inner product spaces of dimension less than $\dim V>1$. Suppose $T\in\ope{V}$ is self-adjoint. By Theorem \ref{trm:eigenSelfAdjoint}, we may let $v$ be an eigenvector of $T$. It follows that $u=v/\norm{v}$ is a normal eigenvector of $T$. Let $U=\spn(u)$. Then $U$ is a subspace of $V$ that is invariant under $T$, so we have by Theorem \ref{trm:invariantSelfAdjointc} that $T|_{U^\perp}\in\ope{U^\perp}$ is self-adjoint. But since $\dim U^\perp=\dim V-\dim U=\dim V-1$, we have by the inductive hypothesis that there is an orthonormal basis of $U^\perp$ consisting of eigenvectors of $T|_{U^\perp}$. Adjoining $u$ to this list gives an orthonormal basis of $V$ consisting of eigenvectors of $T$, as desired.\par
            Second, suppose that $V$ has an orthonormal basis $e_1,\dots,e_n$ consisting of eigenvectors of $T$. Then since
            \begin{equation*}
                Te_j = 0e_1+\cdots+0e_{j-1}+\lambda_je_j+0e_{j+1}+\cdots+0e_n
            \end{equation*}
            for all $j$, we have by the definition that $\mat{T,(e_1,\dots,e_n)}$ is diagonal, as desired.\par
            Third, suppose that $T$ has a diagonal matrix $\mat{T}$ with respect to some orthonormal basis of $V$. In a real inner product space, $\overline{\mat{T}}=\mat{T}$. Additionally, any diagonal matrix is equal to its transpose. Thus, $T=T^*$, so $T$ is self-adjoint, as desired.
        \end{proof}
    \end{theorem}
\end{itemize}



\section{Positive Operators and Isometries}
\begin{itemize}
    \item \marginnote{10/11:}\textbf{Positive} ($T\in\ope{V}$): A self-adjoint operator $T\in\ope{V}$ such that
    \begin{equation*}
        \inp{Tv}{v} \geq 0
    \end{equation*}
    for all $v\in V$. \emph{Also known as} \textbf{positive semidefinite} (operator).
    \begin{itemize}
        \item Note that if $V$ is complex, Theorem \ref{trm:selfAdjointTvvReal} implies based on the condition that $\inp{Tv}{v}\geq 0$ for all $v\in V$ that $T$ is self-adjoint. Therefore, in this case, we need not explicitly postulate that $T$ is self-adjoint.
    \end{itemize}
    \item \textbf{Square root} (of $T\in\ope{V}$): An operator $R$ such that $R^2=T$.
    \item The following characterization of positive operators is directly analogous to the characterization of nonnegative complex numbers.
    \begin{theorem}\label{trm:positiveConditions}
        Let $T\in\ope{V}$. Then the following are equivalent.
        \begin{enumerate}[label={\textup{(}\alph*\textup{)}}]
            \item $T$ is positive.
            \item $T$ is self-adjoint and all the eigenvalues of $T$ are nonnegative.
            \item $T$ has a positive square root.
            \item $T$ has a self-adjoint square root.
            \item There exists an operator $R\in\ope{V}$ such that $T=R^*R$.
        \end{enumerate}
        \begin{proof}
            We will prove that $\text{(a)}\Rightarrow\text{(b)}\Rightarrow\text{(c)}\Rightarrow\text{(d)}\Rightarrow\text{(e)}\Rightarrow\text{(a)}$. Let's begin.\par\smallskip
            First, suppose that $T$ is positive. Then by definition, $T$ is self-adjoint. Additionally, let $\lambda\in\F$ be an eigenvalue of $T$. It follows by the definition of positive operators and by the positivity of the inner product that
            \begin{align*}
                0 &\leq \inp{Tv}{v} = \inp{\lambda v}{v} = \lambda\inp{v}{v}\\
                0 &\leq \lambda
            \end{align*}
            as desired.\par
            Second, suppose that $T$ is self-adjoint and all the eigenvalues of $T$ are nonnegative. Since $T$ is self-adjoint, the \hyperref[trm:RealSpectral]{Real} and \hyperref[trm:ComplexSpectral]{Complex} Spectral Theorems imply that there exists an orthonormal basis $e_1,\dots,e_n$ of $V$ consisting of eigenvectors of $T$. Let $\lambda_1,\dots,\lambda_n$ be the corresponding eigenvalues; it follows by hypothesis that $\lambda_j\geq 0$ for all $j$. We now define $R\in\ope{V}$ by
            \begin{equation*}
                Re_j = \sqrt{\lambda_j}e_j
            \end{equation*}
            for all $j$. To prove that $R$ is positive, let $v\in V$ be arbitrary. Suppose $v=a_1e_1+\cdots+a_ne_n$ where $a_1,\dots,a_n\in\F$. Then
            \begin{align*}
                \inp{Rv}{v} &= \inp{R(a_1e_1+\cdots+a_ne_n)}{a_1e_1+\cdots+a_ne_n}\\
                &= \inp{\sqrt{\lambda_1}a_1e_1+\cdots+\sqrt{\lambda_n}a_ne_n}{a_1e_1+\cdots+a_ne_n}\\
                &= \sum_{i=1}^n\sum_{j=1}^n\inp{\sqrt{\lambda_i}a_ie_i}{a_je_j}\\
                &= \sum_{i=1}^n\inp{\sqrt{\lambda_i}a_ie_i}{a_ie_i}\\
                &= \sum_{i=1}^n\sqrt{\lambda_i}\\
                &\geq 0
            \end{align*}
            as desired. Furthermore, $R^2e_j=\lambda_je_j=Te_j$ for each $j$, so by Theorem \ref{trm:mapBasisToBasis}, $R^2=T$, as desired.\par
            Third, suppose that $T$ has a positive square root $R$. Then by the definition of a positive operator, $R$ is self-adjoint as well, as desired.\par
            Fourth, suppose that $T$ has a self-adjoint square root $R$. Since $R$ is self-adjoint, $R=R^*$. Therefore,
            \begin{equation*}
                T = R^2 = R^*R
            \end{equation*}
            as desired.\par
            Fifth, suppose that there exists an operator $R\in\ope{V}$ such that $T=R^*R$. To prove that $T$ is positive, it will suffice to show that it is self-adjoint and that $\inp{Tv}{v}\geq 0$ for all $v\in V$. First off, $T$ is self-adjoint since
            \begin{equation*}
                T^* = (R^*R)^* = R^*(R^*)^* = R^*R = T
            \end{equation*}
            Second, we have that
            \begin{equation*}
                \inp{Tv}{v} = \inp{R^*Rv}{v} = \inp{Rv}{Rv} \geq 0
            \end{equation*}
            for all $v\in V$. Therefore, $T$ is positive, as desired.
        \end{proof}
    \end{theorem}
    \item Since each nonnegative number has a unique nonnegative square root, the next result makes sense by analogy.
    \begin{theorem}\label{trm:posSquareRoot}
        Every positive operator on $V$ has a unique positive square root.
        \begin{proof}
            Let $T$ be a positive operator on $V$, let $v\in V$ be an eigenvector of $T$, let $\lambda\in\F$ be the corresponding eigenvalue, and let $R$ be a positive square root of $T$ (Theorem \ref{trm:positiveConditions} guarantees that at least one such operator exists). Since $T$ is positive, Theorem \ref{trm:positiveConditions} implies that $\lambda\geq 0$. Thus, to prove that $R$ is unique, we will prove that $Rv=\sqrt{\lambda}v$. This will imply that the behavior of $R$ on the eigenvectors of $T$ is uniquely determined. It will follows since there is a basis of $V$ consisting of the eigenvectors of $T$ (by the \hyperref[trm:RealSpectral]{Real} and \hyperref[trm:ComplexSpectral]{Complex} Spectral Theorems), the behavior of $R$ on $V$ (and hence $R$) is uniquely determined. Let's begin.\par\smallskip
            Since $R$ is positive (hence self-adjoint), the \hyperref[trm:RealSpectral]{Real} and \hyperref[trm:ComplexSpectral]{Complex} Spectral Theorems assert that there exists an orthonormal basis $e_1,\dots,e_n$ of $V$ consisting of eigenvectors of $R$. Additionally, because $R$ is positive, the corresponding eigenvalues $\sqrt{\lambda_1},\dots,\sqrt{\lambda_n}$ are nonnegative.\par
            Now let
            \begin{equation*}
                v = a_1e_1+\cdots+a_ne_n
            \end{equation*}
            for $a_1,\dots,a_n\in\F$. Then
            \begin{equation*}
                a_1\lambda e_1+\cdots+a_n\lambda e_n = Tv = R^2v = a_1\lambda_1e_1+\cdots+a_n\lambda_ne_n
            \end{equation*}
            so since $e_1,\dots,e_n$ is linearly independent, $a_j(\lambda-\lambda_j)=0$ for all $j$. It follows that
            \begin{equation*}
                v = \sum_{\{j:\lambda_j=\lambda\}}a_je_j
            \end{equation*}
            so that
            \begin{align*}
                Rv &= \sum_{j=1}^na_j\sqrt{\lambda_j}e_j\\
                &= \sum_{\{j:\lambda_j=\lambda\}}a_j\sqrt{\lambda_j}e_j\\
                &= \sum_{\{j:\lambda_j=\lambda\}}a_j\sqrt{\lambda}e_j\\
                &= \sqrt{\lambda}v
            \end{align*}
            as desired.
        \end{proof}
    \end{theorem}
    \item \textbf{Isometry}: An operator $S\in\ope{V}$ such that
    \begin{equation*}
        \norm{Sv} = \norm{v}
    \end{equation*}
    for all $v\in V$.
    \begin{itemize}
        \item In other words, an isometry is an operator that preserves norms.
    \end{itemize}
    \item \textbf{Orthogonal} (operator): An isometry on a real inner product space.
    \item \textbf{Unitary} (operator): An isometry on a complex inner product space.
    \item Characterizing isometries.
    \begin{theorem}\label{trm:isomConditions}
        Suppose $S\in\ope{V}$. Then the following are equivalent.
        \begin{enumerate}[label={\textup{(}\alph*\textup{)}}]
            \item $S$ is an isometry.
            \item $\inp{Su}{Sv}=\inp{u}{v}$ for all $u,v\in V$.
            \item $Se_1,\dots,Se_n$ is orthonormal for every orthonormal list of vectors $e_1,\dots,e_n$ in $V$.
            \item There exists an orthonormal basis $e_1,\dots,e_n$ of $V$ such that $Se_1,\dots,Se_n$ is orthonormal.
            \item $S^*S=I$.
            \item $SS^*=I$.
            \item $S^*$ is an isometry.
            \item $S$ is invertible and $S^{-1}=S^*$.
        \end{enumerate}
        \begin{proof}
            We will prove that $\text{(a)}\Rightarrow\text{(b)}\Rightarrow\text{(c)}\Rightarrow\text{(d)}\Rightarrow\text{(e)}\Rightarrow\text{(f)}\Rightarrow\text{(g)}\Rightarrow\text{(h)}\Rightarrow\text{(a)}$. Let's begin.\par\smallskip
            First, suppose that $S$ is an isometry. Let $u,v\in V$ be arbitrary. We divide into two cases ($V$ is a real inner product space and $V$ is a complex inner product space). If $V$ is a real inner product space, then
            \begin{align*}
                \inp{Su}{Sv} &= \frac{\norm{Su+Sv}^2-\norm{Su-Sv}^2}{4}\tag*{Exercise \ref{exr:6A19}}\\
                &= \frac{\norm{S(u+v)}^2-\norm{S(u-v)}^2}{4}\\
                &= \frac{\norm{u+v}^2-\norm{u-v}^2}{4}\\
                &= \inp{u}{v}\tag*{Exercise \ref{exr:6A19}}
            \end{align*}
            as desired. On the other hand, if $V$ is a complex vector space, then the proof is symmetric to the above except with the use of Exercise \ref{exr:6A20} instead of Exercise \ref{exr:6A19}.\par
            Second, suppose that $\inp{Su}{Sv}=\inp{u}{v}$ for all $u,v\in V$. Let $e_1,\dots,e_n$ be an orthonormal list of vectors in $V$. Then by hypothesis,
            \begin{equation*}
                \inp{Se_i}{Se_j} = \inp{e_i}{e_j} = \delta_{ij}
            \end{equation*}
            for all $1\leq i,j\leq n$, proving that $Se_1,\dots,Se_n$ is orthonormal, as desired.\par
            Third, suppose that $Se_1,\dots,Se_n$ is orthonormal for every orthonormal list of vectors $e_1,\dots,e_n$ in $V$. By Theorem \ref{trm:orthonormalExists}, there exists an orthonormal basis $e_1,\dots,e_n$ of $V$. It follows by hypothesis that $Se_1,\dots,Se_n$ is orthonormal, as desired.\par
            Fourth, suppose that there exists an orthonormal basis $e_1,\dots,e_n$ of $V$ such that $Se_1,\dots,Se_n$ is orthonormal. Then
            \begin{equation*}
                \inp{S^*Se_j}{e_k} = \inp{Se_j}{Se_k} = \delta_{jk} = \inp{e_j}{e_k}
            \end{equation*}
            for all $1\leq j,k\leq n$. It follows that if $u,v\in V$, then
            \begin{align*}
                \inp{S^*Su}{v} &= \inp{S^*S(a_1e_1+\cdots+a_ne_n)}{b_1e_1+\cdots+b_ne_n}\\
                &= \inp{S^*Sa_1e_1}{b_1e_1}+\cdots+\inp{S^*Sa_ne_n}{b_ne_n}\\
                &= \inp{a_1e_1}{b_1e_1}+\cdots+\inp{a_ne_n}{b_ne_n}\\
                &= \inp{a_1e_1+\cdots+a_ne_n}{b_1e_1+\cdots+b_ne_n}\\
                &= \inp{u}{v}
            \end{align*}
            Therefore, $S^*S=I$, as desired.\par
            Fifth, suppose that $S^*S=I$. Then by Exercise \ref{exr:3D10}, $SS^*=I$, as desired.\par
            Sixth, suppose that $SS^*=I$. To prove that $S^*$ is an isometry, it will suffice to show that $\norm{S^*v}=\norm{v}$ for all $v\in V$. Let $v\in V$ be arbitrary. Then
            \begin{equation*}
                \norm{S^*v}^2 = \inp{S^*v}{S^*v} = \inp{SS^*v}{v} = \inp{v}{v} = \norm{v}^2
            \end{equation*}
            Taking square roots yields the desired equality.\par
            Seventh, suppose that $S^*$ is an isometry. It follows by our previous chain of proofs that $(S^*)^*S^*=SS^*=I$ and $S^*(S^*)^*=S^*S=I$. Therefore, $S$ is invertible with inverse $S^{-1}=S^*$, as desired.\par
            Eighth, suppose that $S$ is invertible and $S^{-1}=S^*$. Then if $v\in V$, we have that
            \begin{equation*}
                \norm{Sv}^2 = \inp{Sv}{Sv} = \inp{S^*Sv}{v} = \inp{S^{-1}Sv}{v} = \inp{v}{v} = \norm{v}^2
            \end{equation*}
            Taking square roots yields the desired equality.
        \end{proof}
    \end{theorem}
    \item It follows from (e) and (f) that every isometry is normal.
    \item Thus, characterizations of normal operators (e.g., the \hyperref[trm:ComplexSpectral]{Complex Spectral Theorem}) can be used to give descriptions of isometries.
    \begin{theorem}\label{trm:isomEigenvalues}
        Suppose $V$ is a complex inner product space and $S\in\ope{V}$. Then the following are equivalent.
        \begin{enumerate}[label={\textup{(}\alph*\textup{)}}]
            \item $S$ is an isometry.
            \item There is an orthonormal basis of $V$ consisting of eigenvectors of $S$ whose corresponding eigenvalues all have absolute value 1.
        \end{enumerate}
        \begin{proof}
            Suppose first that $S$ is an isometry. Then by the \hyperref[trm:ComplexSpectral]{Complex Spectral Theorem}, there is an orthonormal basis $e_1,\dots,e_n$ of $V$ consisting of the eigenvectors of $S$. Let $\lambda_1,\dots,\lambda_n$ be the corresponding eigenvalues. Then for each $j=1,\dots,n$, we have that
            \begin{equation*}
                |\lambda_j| = \norm{\lambda_je_j} = \norm{Se_j} = \norm{e_j} = 1
            \end{equation*}
            as desired.\par
            Now suppose that there is an orthonormal basis $e_1,\dots,e_n$ of $V$ consisting of eigenvectors of $S$ whose corresponding eigenvalues $\lambda_1,\dots,\lambda_n$ all have absolute value 1. Let $v\in V$ be arbitrary. Then by Theorem \ref{trm:linCombOrthonormal}, we have that
            \begin{align*}
                v &= \inp{v}{e_1}e_1+\cdots+\inp{v}{e_n}e_n&
                \norm{v}^2 &= |\inp{v}{e_1}|^2+\cdots+|\inp{v}{e_n}|^2
            \end{align*}
            It follows that
            \begin{align*}
                Sv &= \inp{v}{e_1}Se_1+\cdots+\inp{v}{e_n}Se_n\\
                &= \inp{v}{e_1}\lambda_1e_1+\cdots+\inp{v}{e_n}\lambda_ne_n
            \end{align*}
            Thus, we have that
            \begin{align*}
                \norm{Sv}^2 &= \inp{\inp{v}{e_1}\lambda_1e_1+\cdots+\inp{v}{e_n}\lambda_ne_n}{\inp{v}{e_1}\lambda_1e_1+\cdots+\inp{v}{e_n}\lambda_ne_n}\\
                &= \inp{\inp{v}{e_1}\lambda_1e_1}{\inp{v}{e_1}\lambda_1e_1}+\cdots+\inp{\inp{v}{e_n}\lambda_ne_n}{\inp{v}{e_n}\lambda_ne_n}\\
                &= \inp{v}{e_1}\lambda_1\cdot\overline{\inp{v}{e_1}\lambda_1}\cdot\inp{e_1}{e_1}+\cdots+\inp{v}{e_n}\lambda_n\cdot\overline{\inp{v}{e_n}\lambda_n}\cdot\inp{e_n}{e_n}\\
                &= \inp{v}{e_1}\lambda_1\cdot\overline{\inp{v}{e_1}}\bar{\lambda}_1\cdot 1+\cdots+\inp{v}{e_n}\lambda_n\cdot\overline{\inp{v}{e_n}}\bar{\lambda}_n\cdot 1\\
                &= |\inp{v}{e_1}|^2|\bar{\lambda}_1|^2+\cdots+|\inp{v}{e_n}|^2|\bar{\lambda}_n|^2\\
                &= |\inp{v}{e_1}|^2\cdot 1+\cdots+|\inp{v}{e_n}|^2\cdot 1\\
                &= \norm{v}^2
            \end{align*}
            Taking square roots yields the desired equality.
        \end{proof}
    \end{theorem}
\end{itemize}



\section{Polar Decomposition and Singular Value Decomposition}
\begin{itemize}
    \item \marginnote{10/16:}\textbf{Square root} (of a positive $T\in\ope{V}$): The unique positive operator $R\in\ope{V}$ such that $R^2=T$. \emph{Denoted by} $\bm{\sqrt{T}}$.
    \begin{itemize}
        \item The existence of such an operator is justified by Theorem \ref{trm:posSquareRoot}.
    \end{itemize}
    \item \marginnote{10/18:}Continuing with our analogy between $\C$ and $\ope{V}$, we now prove an analogous theorem to the decomposition of any complex number $z$ into the form $z=(z/|z|)|z|=(z/|z|)\sqrt{\bar{z}z}$, where $z/|z|$ (as an element of the unit circle) is analogous to an isometry, and $\bar{z}$ is analogous to the adjoint.
    \begin{theorem}[Polar Decomposition]\label{trm:polarDecomp}
        Suppose $T\in\ope{V}$. Then there exists an isometry $S\in\ope{V}$ such that
        \begin{equation*}
            T = S\sqrt{T^*T}
        \end{equation*}
        \begin{lemma*}\label{lem:normTroot}
            If $v\in V$, then
            \begin{equation*}
                \norm{Tv} = \norm{\sqrt{T^*T}v}
            \end{equation*}
            \begin{proof}
                Let $v\in V$ be arbitrary. Then
                \begin{align*}
                    \norm{Tv}^2 &= \inp{Tv}{Tv}\\
                    &= \inp{T^*Tv}{v}\\
                    &= \inp{\sqrt{T^*T}\sqrt{T^*T}v}{v}\\
                    &= \inp{\sqrt{T^*T}v}{\sqrt{T^*T}v}\\
                    &= \norm{\sqrt{T^*T}v}^2
                \end{align*}
                where the third equality holds because $T^*T$ is positive by Theorem \ref{trm:positiveConditions} and thus has a positive square root, and the fourth equality holds because $\sqrt{T^*T}$ is positive and thus is self-adjoint by definition. Taking square roots of the above gives the desired inequality.
            \end{proof}
        \end{lemma*}
        \begin{proof}[Proof of Theorem {\hyperref[trm:polarDecomp]{7.21}}]
            For this proof, we will first define a map $S_1:\range\sqrt{T^*T}\to\range T$. We will then prove that it is a well-defined function and that it is a linear map. $S_1$ thus has the desired property; all that remains is to extend it to an isometry. To do so, we define $S_2:(\range\sqrt{T^*T})^\perp\to(\range T)^\perp$ so that $S$, defined as $S_1$ on the appropriate domain and $S_2$ on its complement, is an isometry. Let's begin.\par\medskip
            Let $S_1:\range\sqrt{T^*Tv}\to\range T$ be defined by
            \begin{equation*}
                S_1(\sqrt{T^*T}v) = Tv
            \end{equation*}
            for all $\sqrt{T^*T}v\in\range\sqrt{T^*T}$.\par
            To prove that $S_1$ is a function, it will suffice to show that if $\sqrt{T^*T}v_1=\sqrt{T^*T}v_2$, then $Tv_1=Tv_2$. But if $\sqrt{T^*T}v_1=\sqrt{T^*T}v_2$, then
            \begin{align*}
                \norm{Tv_1-Tv_2} &= \norm{T(v_1-v_2)}\\
                &= \norm{\sqrt{T^*T}(v_1-v_2)}\tag*{\hyperref[lem:normTroot]{Lemma}}\\
                &= \norm{\sqrt{T^*T}v_1-\sqrt{T^*T}v_2}\\
                &= 0
            \end{align*}
            Thus, by Theorem \ref{trm:normPropertiesa}, $Tv_1-Tv_2=0$, so $Tv_1=Tv_2$, as desired.\par
            To prove that $S_1$ is a linear map, it will suffice to show that $S_1(\alpha\sqrt{T^*T}v)=\alpha S_1(\sqrt{T^*T}v)$ where $\alpha\in\F$ and $S_1(\sqrt{T^*T}v_1+\sqrt{T^*T}v_2)=S_1(\sqrt{T^*T}v_1)+S_1(\sqrt{T^*T}v_2)$. But since $\sqrt{T^*T}$ and $T$ are both linear maps themselves, we have that
            \begin{align*}
                S_1(\alpha\sqrt{T^*T}v) &= S_1(\sqrt{T^*T}(\alpha v))&
                    S_1(\sqrt{T^*T}v_1+\sqrt{T^*T}v_2) &= S_1(\sqrt{T^*T}(v_1+v_2))\\
                &= T(\alpha v)&
                    &= T(v_1+v_2)\\
                &= \alpha Tv&
                    &= Tv_1+Tv_2\\
                &= \alpha S_1(\sqrt{T^*T}v)&
                    &= S_1(\sqrt{T^*T}v_1)+S_1(\sqrt{T^*T}v_2)
            \end{align*}
            as desired.\par
            To prove that $S_1$ is an isometry, it will suffice to show that $\norm*{S_1(\sqrt{T^*T}v)}=\norm*{\sqrt{T^*T}v}$ for all $\sqrt{T^*T}v\in\range\sqrt{T^*T}$. But if $\sqrt{T^*T}v\in\range\sqrt{T^*T}$, then
            \begin{align*}
                \norm{S_1(\sqrt{T^*T}v)} &= \norm{Tv}\\
                &= \norm{\sqrt{T^*T}v}
            \end{align*}
            where the first equality holds by the definition of $S_1$, and the second holds by the \hyperref[lem:normTroot]{Lemma}.\par\smallskip
            We now build up to our definition of $S_2$. For starters, notice that it follows from the \hyperref[lem:normTroot]{Lemma} that $S_1$ is injective much the same way it followed that $S_1$ was a function. Consequently, Theorem \ref{trm:nullSpaceInjective} asserts that $\nul S_1=\{0\}$. Thus, since $S_1\in\lin{\range\sqrt{T^*T}}{\range T}$, we have by the \hyperref[trm:fundamentalTheoremLinearMaps]{Fundamental Theorem of Linear Maps} that
            \begin{align*}
                \dim\range\sqrt{T^*T} &= \dim\nul S_1+\dim\range S_1\\
                &= 0+\dim\range T\\
                &= \dim\range T
            \end{align*}
            Thus, since $\range\sqrt{T^*T}\subset V$ and $\range T\subset V$, we have that
            \begin{align*}
                \dim(\range T)^\perp &= \dim V-\dim\range T\tag*{Theorem \ref{trm:dimPerp}}\\
                &= \dim V-\dim\range\sqrt{T^*T}\\
                &= \dim(\range\sqrt{T^*T})^\perp\tag*{Theorem \ref{trm:dimPerp}}
            \end{align*}
            It follows that we can choose orthonormal bases $e_1,\dots,e_m$ and $f_1,\dots,f_m$ of $(\range\sqrt{T^*T})^\perp$ and $(\range T)^\perp$ of equal length. Let $S_2:(\range\sqrt{T^*T})^\perp\to(\range T)^\perp$ be the unique linear transformation such that $Te_j=f_j$ for each $j=1,\dots,n$ implied to exist by Theorem \ref{trm:mapBasisToBasis}. Note that $S_2$ is also an isometry since if $x\in(\range\sqrt{T^*T})^\perp$, then
            \begin{align*}
                \norm{S_2x}^2 &= \norm{S_1(a_1e_1+\cdots+a_me_m)}^2\\
                &= \norm{a_1f_1+\cdots+a_mf_m}^2\\
                &= |a_1|^2+\cdots+|a_m|^2\tag*{Theorem \ref{trm:normOrthoLnlComb}}\\
                &= \norm{a_1e_1+\cdots+a_mf_m}^2\tag*{Theorem \ref{trm:normOrthoLnlComb}}\\
                &= \norm{x}^2
            \end{align*}
            where taking square roots yields the desired equality.\par
            We are now ready to define $S\in\ope{V}$. Let $v\in V$ be arbitrary. It follows by Theorem \ref{trm:perpDirectSum} that we can uniquely decompose $v$ into a sum $v=u+w$ where $u\in\range\sqrt{T^*T}$ and $v\in(\range\sqrt{T^*T})^\perp$. Thus, we define
            \begin{equation*}
                Sv = S_1u+S_2w
            \end{equation*}
            We could (but will not) explicitly show based on the previously proven properties that $S$ is well-defined and linear. We will, however, show that $S$ is an isometry: for any $v\in V$,
            \begin{align*}
                \norm{Sv}^2 &= \norm{S_1u+S_2v}^2\\
                &= \norm{S_1u}^2+\norm{S_2w}\tag*{\hyperref[trm:pythagorean]{Pythagorean Theorem}}\\
                &= \norm{u}^2+\norm{w}^2\\
                &= \norm{v}^2\tag*{\hyperref[trm:pythagorean]{Pythagorean Theorem}}
            \end{align*}
            Lastly, we have by its definition that for any $v\in V$,
            \begin{equation*}
                (S\sqrt{T^*T})v = S(\sqrt{T^*T}v) = S_1(\sqrt{T^*T}v) = Tv
            \end{equation*}
            so $T=S\sqrt{T^*T}$, as desired.
        \end{proof}
    \end{theorem}
    \item \marginnote{10/21:}The main conclusion from the Polar Decomposition is that \emph{any} linear operator, no matter how ill-defined, can be decomposed into the product of an isometry and a positive operator, two very well characterized classes of operators.
    \begin{itemize}
        \item In particular, if $\F=\C$, then $T$ is the product of two operators, both of which are orthonormally diagonalizable (though not necessarily with respect to the same orthonormal bases).
    \end{itemize}
    \item \textbf{Singular values} (of $T\in\ope{V}$): The eigenvalues of $\sqrt{T^*T}$, with each value $\lambda$ repeated $\dim E(\lambda,\sqrt{T^*T})$ times.
    \item The singular values of $T$ are all nonnegative (because $\sqrt{T^*T}$ is a positive operator [see Theorem \ref{trm:positiveConditions}]). 
    \item Each $T\in\ope{V}$ has $\dim V$ singular values (because $\sqrt{T^*T}$ is positive, hence self-adjoint, hence $\sqrt{T^*T}$ has a diagonal matrix [see the \hyperref[trm:RealSpectral]{Real} Spectral Theorem], hence $\sqrt{T^*T}$ has $\dim V$ distinct eigenvalues).
    \item We now show that every operator on $V$ can be described in terms of its singular values and two orthonormal bases on $V$.
    \begin{theorem}[Singular Value Decomposition]\label{trm:SVD}
        Suppose $T\in\ope{V}$ has singular values $s_1,\dots,s_n$. Then there exist orthonormal bases $e_1,\dots,e_n$ and $f_1,\dots,f_n$ of $V$ such that
        \begin{equation*}
            Tv = s_1\inp{v}{e_1}f_1+\cdots+s_n\inp{v}{e_n}f_n
        \end{equation*}
        for every $v\in V$.
        \begin{proof}
            Applying the \hyperref[trm:RealSpectral]{Real} Spectral Theorem to the self-adjoint operator $\sqrt{T^*T}$ reveals that $V$ has an orthonormal basis $e_1,\dots,e_n$ of eigenvectors of $\sqrt{T^*T}$. Therefore, if we let $v\in V$ be arbitrary, then we have that
            \begin{align*}
                Tv &= (S\sqrt{T^*T})v\tag*{\hyperref[trm:polarDecomp]{Polar Decomposition}}\\
                &= S(\sqrt{T^*T}(\inp{v}{e_1}e_1+\cdots+\inp{v}{e_n}e_n))\tag*{Theorem \ref{trm:linCombOrthonormal}}\\
                &= S(\inp{v}{e_1}\sqrt{T^*T}e_1+\cdots+\inp{v}{e_n}\sqrt{T^*T}e_n)\\
                &= S(\inp{v}{e_1}s_1e_1+\cdots+\inp{v}{e_n}s_ne_n)\\
                &= s_1\inp{v}{e_1}Se_1+\cdots+s_n\inp{v}{e_n}Se_n\\
                &= s_1\inp{v}{e_1}f_1+\cdots+s_n\inp{v}{e_n}f_n\tag*{Theorem \ref{trm:isomConditions}}
            \end{align*}
            where $s_1,\dots,s_n$ are the singular values of $T$ (the eigenvalues of $\sqrt{T^*T}$) and $f_1,\dots,f_n$ is another orthonormal basis of $V$.
        \end{proof}
    \end{theorem}
    \item If $e_1,\dots,e_n$ and $f_1,\dots,f_n$ are orthonormal bases of $V$ that satisfy the \hyperref[trm:SVD]{Singular Value Decomposition} for some operator $T$, then $Te_j=s_jf_j$ for each $j=1,\dots,n$.
    \begin{itemize}
        \item In other words, every operator on $V$ has a diagonal matrix with respect to some orthonormal \emph{bases} (plural) of $V$.
    \end{itemize}
    \item The \hyperref[trm:SVD]{Singular Value Decomposition} has many applications, especially in the realm of computational linear algebra, where working with $T^*T$ is much easier than working with $\sqrt{T^*T}$. A powerful tool in this pursuit is the following.
    \begin{theorem}
        Suppose $T\in\ope{V}$. Then the singular values of $T$ are the nonnegative square roots of the eigenvalues of $T^*T$, with each eigenvalue $\lambda$ repeated $\dim E(\lambda,T^*T)$ times.
        \begin{proof}
            Since $T^*T$ is positive and self-adjoint, we have by the hyperref[trm:RealSpectral]{Real} Spectral Theorem that there exists an orthonormal basis $e_1,\dots,e_n$ of $V$ and nonnegative numbers $\lambda_1,\dots,\lambda_n$ such that $T^*Te_j=\lambda_je_j$ for each $j=1,\dots,n$. It follows since $\sqrt{T^*T}$ is also a positive, self-adjoint operator that its eigenvalues (which exist) must be of the form $\sqrt{\lambda_1},\dots,\sqrt{\lambda_n}$ to satisfy $(\sqrt{T^*T})^2=T^*T$ and to be nonnegative.
        \end{proof}
    \end{theorem}
\end{itemize}




\end{document}