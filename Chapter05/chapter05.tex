\documentclass[../main.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter: #1}{}}
\setcounter{chapter}{4}

\begin{document}




\chapter{Eigenvalues, Eigenvectors, and Invariant Subspaces}
\section{Invariant Subspaces}
\begin{itemize}
    \item \marginnote{9/8:}Let $T\in\ope{V}$, and let $V$ be decomposable into a direct sum of proper subspaces as follows.
    \begin{equation*}
        V = U_1\oplus\cdots\oplus U_m
    \end{equation*}
    \begin{itemize}
        \item To understand $T$, we need only understand each each \textbf{restriction} of $T$ to a $U_j$.
        \item Since $T|_{U_j}$ may not map $U_j$ onto itself in every case, to use operator-based tools, we need to consider only direct sum decompositions into subspaces that $T$ maps onto themselves, or \textbf{invariant subspace}.
    \end{itemize}
    \item \textbf{Invariant subspace} (of $V$ under $T$): A subspace $U$ of $V$ such that $u\in U$ implies $Tu\in U$, where $T\in\ope{V}$.
    \begin{itemize}
        \item In other words, $U$ is invariant under $T$ iff $T|_U\in\ope{U}$.
    \end{itemize}
    \item Some invariant subspaces under $T\in\ope{V}$: $\{0\}$, $V$, $\text{null }T$, and $\text{range }T$.
    \item \textbf{Invariant subspace problem}: The most famous unsolved problem in functional analysis, dealing with invariant subspaces of operators on infinite-dimensional vector spaces.
    \item To begin our study of invariant subspaces, we consider the simplest possible type of invariant subspace: those with dimension 1.
    \item Every 1-dimensional subspace of $V$ is of the form $\spn(v)$ for some $v\in V$.
    \begin{itemize}
        \item If $\spn(v)$ is invariant under $T\in\ope{V}$, then $Tv\in\spn(v)$.
        \item If $Tv\in\spn(v)$, then there exists $\lambda\in\F$ such that $Tv=\lambda v$.
    \end{itemize}
    \item \textbf{Eigenvalue} (of $T$): A number $\lambda\in\F$ such that there exists a nonzero vector $v\in V$ satisfying the equation $Tv=\lambda v$. \emph{Also known as} \textbf{characteristic value}.
    \item \dq{$T$ has a 1-dimensional invariant subspace if and only if $T$ has an eigenvalue}{134}
    \item We now give some conditions $\lambda$ can satisfy to be deemed an eigenvalue.
    \begin{theorem}\label{trm:eigenConditions}
        Suppose $V$ is finite-dimensional, $T\in\ope{V}$, $I\in\ope{V}$ is the identity operator on $V$, and $\lambda\in\F$. Then the following are equivalent.
        \begin{enumerate}[label={\textup{(}\alph*\textup{)}}]
            \item $\lambda$ is an eigenvalue of $T$.
            \item $T-\lambda I$ is not injective.
            \item $T-\lambda I$ is not surjective.
            \item $T-\lambda I$ is not invertible.
        \end{enumerate}
        \begin{proof}
            Suppose first that $\lambda$ is an eigenvalue of $T$. Then
            \begin{align*}
                Tv &= \lambda v\\
                Tv &= \lambda Iv\\
                Tv-\lambda Iv &= 0\\
                (T-\lambda I)v &= 0
            \end{align*}
            for some $v\in V$ such that $v\neq 0$. It follows that $v\in\nul(T-\lambda I)$, so by Theorem \ref{trm:nullSpaceInjective}, $T-\lambda I$ is not injective, as desired. The proof is symmetric in the other direction. Therefore, conditions (a) and (b) are equivalent.\par
            To prove that (a), (b), (c), and (d) are equivalent at this point, it will suffice to show that (b), (c), and (d) are equivalent. But we have this by Theorem \ref{trm:invertibleInjectiveSurjective}, as desired.
        \end{proof}
    \end{theorem}
    \item \textbf{Eigenvector} (of $T$): A nonzero vector $v\in V$ such that there exists a $\lambda\in\F$ satisfying the equation $Tv=\lambda v$.
    \item Since $Tv=\lambda v$ iff $(T-\lambda I)v=0$, \dq{a vector $v\in V$ with $v\neq 0$ is an eigenvector of $T$ corresponding to $\lambda$ if and only if $v\in\text{null}(T-\lambda I)$}{135}
    \item Eigenvectors corresponding to distinct eigenvalues are linearly independent.
    \begin{theorem}\label{trm:lnlIndependentEigenvectors}
        Let $T\in\ope{V}$. Suppose $\lambda_1,\dots,\lambda_m$ are distinct eigenvalues of $T$ and $v_1,\dots,v_m$ are corresponding eigenvectors. Then $v_1,\dots,v_m$ is linearly independent.
        \begin{proof}
            Suppose for the sake of contradiction that $v_1,\dots,v_m$ is linearly dependent. Then by the \hyperref[lem:linearDependenceLemma]{Linear Dependence Lemma}, we may let $k$ be the smallest positive integer such that $v_k\in\spn(v_1,\dots,v_{k-1})$. It follows that
            \begin{equation*}
                v_k = a_1v_1+\cdots+a_{k-1}v_{k-1}
            \end{equation*}
            for some $a_1,\dots,a_{k-1}\in\F$. Thus, applying $T$, we have that
            \begin{align*}
                Tv_k &= a_1Tv_1+\cdots+a_{k-1}Tv_{k-1}\\
                \lambda_kv_k &= a_1\lambda_1v_1+\cdots+a_{k-1}\lambda_{k-1}v_{k-1}
            \end{align*}
            If we multiply the first equation by $\lambda_k$ and subtract the above equation from it, we get that
            \begin{equation*}
                0 = a_1(\lambda_k-\lambda_1)v_1+\cdots+a_{k-1}(\lambda_k-\lambda_{k-1})v_{k-1}
            \end{equation*}
            But since $k$ is the smallest positive integer $j$ such that $v_j\in\spn(v_1,\dots,v_{j-1})$, we know that $v_1,\dots,v_{k-1}$ are linearly independent. Thus, $a_1(\lambda_k-\lambda_1)=\cdots=a_{k-1}(\lambda_k-\lambda_{k-1})=0$. More specifically, since all eigenvalues are distinct (i.e., $\lambda_k-\lambda_j\neq 0$ for any $j=1,\dots,k-1$), we must have that $a_1=\cdots=a_{k-1}=0$. But this implies that
            \begin{align*}
                v_k &= a_1v_1+\cdots+a_{k-1}v_{k-1}\\
                &= 0
            \end{align*}
            contradicting the fact that $v_k$, as an eigenvector, is nonzero.
        \end{proof}
    \end{theorem}
    \item We now put a bound on the number of eigenvalues.
    \begin{theorem}
        Suppose $V$ is finite-dimensional. Then each operator on $V$ has at most $\dim V$ distinct eigenvalues.
        \begin{proof}
            Let $T\in\ope{V}$ have distinct eigenvalues $\lambda_1,\dots,\lambda_m$ and corresponding eigenvectors $v_1,\dots,v_m$. Then by Theorem \ref{trm:lnlIndependentEigenvectors}, $v_1,\dots,v_m$ is linearly independent. It follows by Theorem \ref{trm:linearIndependent-Spanning} that $m\leq\dim V$
        \end{proof}
    \end{theorem}
    \item \textbf{Restriction operator} (of $T:V\to W$ to $U\subset V$): The function $T|_U:U\to W$ defined by $T|_U(u)=Tu$ for all $u\in U$. \emph{Denoted by} $\bm{T|_U}$.
    \begin{itemize}
        \item The fact that $U$ is invariant under $T$ is what allows us to consider $T|_U$ to be in $\ope{U}$ as opposed to just $\ope{V}$.
    \end{itemize}
    \item \textbf{Quotient operator}: The operator $T/U\in\ope{V/U}$ defined by $(T/U)(v+U)=Tv+U$ for all $v\in V$.
    \item \textcite{bib:Axler} verifies that the restriction operator and the quotient operator actually \emph{are} operators, in general, as defined.
\end{itemize}



\section{Eigenvectors and Upper-Triangular Matrices}
\begin{itemize}
    \item \marginnote{9/10:}If an operator $T\in\ope{V}$, then $TT=T^2\in\ope{V}$.
    \item $\bm{T^m}$: The operator $T^m\in\ope{V}$ defined by
    \begin{equation*}
        T^m = \underbrace{T\cdots T}_\text{$m$ times}
    \end{equation*}
    where $T\in\ope{V}$, $m\in\N$.
    \item $\bm{T^0}$: The identity operator $I\in\ope{V}$, where $T\in\ope{V}$.
    \item $\bm{T^{-m}}$: The operator $T^{-m}\in\ope{V}$ defined by
    \begin{equation*}
        T^{-m} = (T^{-1})^m
    \end{equation*}
    where $T\in\ope{V}$ is invertible with inverse $T^{-1}$, and $m\in\N$.
    \item It follows from these definitions that
    \begin{align*}
        T^mT^n &= T^{m+n}&
        (T^m)^n &= T^{mn}
    \end{align*}
    for any $m,n\in\Z$ if $T$ is invertible and for any $m,n\in\N$ if $T$ is not invertible.
    \item $\bm{p(T)}$: The operator defined by
    \begin{equation*}
        p(T) = a_0I+a_1T+\cdots+a_mT^m
    \end{equation*}
    where $T\in\ope{V}$, and $p\in\pol{\F}$ is defined by $p(z) = a_0+a_1z+\cdots+a_mz^m$ for all $z\in\F$.
    \item $f:\pol{\F}\to\ope{V}$ defined by $p\mapsto p(T)$ is linear.
    \item \textbf{Product} (of $p,q\in\pol{\F}$): The polynomial $pq\in\pol{\F}$ defined by $(pq)(z)=p(z)q(z)$ for all $z\in\F$.
    \item Multiplicative properties of $p(T)$.
    \begin{theorem}\label{trm:polTMultiplicative}
        Suppose $p,q\in\pol{\F}$ and $T\in\ope{V}$. Then
        \begin{enumerate}[label={\textup{(}\alph*\textup{)}}]
            \item $(pq)(T)=p(T)q(T)$.
            \begin{proof}
                Suppose $p(z)=\sum_{j=0}^ma_jz^j$ and $q(z)=\sum_{k=0}^nb_kz^k$ for all $z\in\F$. Then
                \begin{equation*}
                    (pq)(z) = \sum_{j=0}^m\sum_{k=0}^na_jb_kz^{j+k}
                \end{equation*}
                so
                \begin{align*}
                    (pq)(T) &= \sum_{j=0}^m\sum_{k=0}^na_jb_kT^{j+k}\\
                    &= \left( \sum_{j=0}^ma_jT^j \right)\left( \sum_{k=0}^nb_kT^k \right)\\
                    &= p(T)q(T)
                \end{align*}
                as desired.
            \end{proof}
            \item $p(T)q(T)=q(T)p(T)$.
            \begin{proof}
                It follows from part (a) that $p(T)q(T)=(pq)(T)=(qp)(T)=q(T)p(T)$, as desired.
            \end{proof}
        \end{enumerate}
    \end{theorem}
    \item We now prove a central result concerning operators on complex vector spaces.
    \begin{theorem}\label{trm:eigenExists}
        Every operator on a finite-dimensional, nonzero, complex vector space has an eigenvalue.
        \begin{proof}
            Let $V$ be nonzero complex vector space of dimension $n$, let $T\in\ope{V}$, and let $v\in V$ be nonzero. Then since $\len(v,Tv,T^2v,\dots,T^nv)>\dim V$, Theorem \ref{trm:linearIndependent-Spanning} implies that $v,Tv,\dots,T^nv$ is not linearly independent. Thus, there exist $a_0,\dots,a_n\in\F$, not all 0, such that
            \begin{equation*}
                0 = a_0v+a_1Tv+\cdots+a_nT^nv
            \end{equation*}
            Additionally, we have that $a_1,\dots,a_n$ do not all equal zero (suppose they did; then $0=a_0v$, so $a_0=0$ because $v\neq 0$; this would imply that $a_0=\cdots=a_n=0$, a contradiction). Thus, if we consider the (nonconstant, by the previous result) polynomial with coefficients equal to the $a$'s, Theorem \ref{trm:polFactorization} implies that
            \begin{equation*}
                a_0+a_1z+\cdots+a_nz^n = c(z-\lambda_1)\cdots(z-\lambda_m)
            \end{equation*}
            for all $z\in\C$, where $c\in\C$ is nonzero and each $\lambda_j\in\C$\footnote{Note that $m\neq n$ necessarily, because $a_n$ may equal 0.}. It follows that
            \begin{align*}
                0 &= a_0v+a_1Tv+\cdots+a_nT^nv\\
                &= (a_0I+a_1T+\cdots+a_nT^n)v\\
                &= c(T-\lambda_1I)\cdots(T-\lambda_mI)v\tag*{Theorem \ref{trm:polTMultiplicative}}
            \end{align*}
            Each $T-\lambda_jI$ is a linear operator in its own right. As a linear map, we have by Theorem \ref{trm:linSendsZero} that $(T-\lambda_jI)0=0$ for all $j=1,\dots,m$. However, $v\neq 0$, meaning that there exists $1\leq k\leq m$ such that $T-\lambda_kI$ maps its nonzero argument, which we may call $v'$ and know to be the result of all of the operators to its right being applied successively to $v$ via function composition, to 0 as well; every operator to the left of $T-\lambda_kI$ will then map the result to 0, resulting in the final equivalence. Thus, $(T-\lambda_kI)v'=(T-\lambda_kI)0$ but $v'\neq 0$ for this operator, meaning that it is not injective. But if $T-\lambda_kI$ is not injective, Theorem \ref{trm:eigenConditions} implies that $\lambda_k$ is an eigenvalue of $T$, as desired.
        \end{proof}
    \end{theorem}
    \item \textbf{Matrix} (of $T\in\ope{V}$ with respect to the basis $v_1,\dots,v_n$ of $V$): The $n$-by-$n$ matrix $\mat{T}$ whose entries $A_{j,k}$ are defined by
    \begin{equation*}
        Tv_k = A_{1,k}v_1+\cdots+A_{n,k}v_n
    \end{equation*}
    \begin{itemize}
        \item If the basis is not clear from context, then the notation $\mat{T,(v_1,\dots,v_n)}$ is used.
    \end{itemize}
    \item \dq{A central goal of linear algebra is to show that given an operator $T\in\ope{V}$, there exists a basis of $V$ with respect to which $T$ has a reasonably simple [sparse] matrix}{146}
    \begin{itemize}
        \item For example, given just Theorems \ref{trm:columnBasis} and \ref{trm:eigenExists}, we know that we can choose a basis of $V$ such that the first column of $\mat{T}$ for an arbitrary $T$ will have an eigenvalue $\lambda$ of $T$ in the first row and zeros everywhere else. Specifically, let $v_1$ be the eigenvector corresponding to $\lambda$ (guaranteed to exist by Theorem \ref{trm:eigenExists}). Then by Theorem \ref{trm:columnBasis},
        \begin{align*}
            \mat{T}_{\cdot,1} &= \mat{Tv_1}\\
            &= \mat{\lambda v_1}\\
            &= \lambda\mat{v_1}+0\mat{v_2}+\cdots+0\mat{v_n}
        \end{align*}
        where we extend $v_1$ into a basis $v_1,\dots,v_n$ of $V$.
    \end{itemize}
    \item \textbf{Diagonal} (of a square matrix): The entries along the line from the upper left corner to the bottom right corner.
    \item \textbf{Upper-triangular matrix}: A matrix such that all the entries below the diagonal equal 0. \emph{Denoted by}
    \begin{equation*}
        \begin{pmatrix}
            \lambda_1 &  & *\\
             & \ddots & \\
            0 &  & \lambda_n\\
        \end{pmatrix}\footnotemark
    \end{equation*}
    \footnotetext{We often use $*$ to denote matrix entries that we do not know about or that are irrelevant to the questions being discussed.}
    \item \marginnote{9/22:}Conditions on $T$ that imply that $\mat{T}$ is upper triangular.
    \begin{theorem}\label{trm:upperTriangularConditions}
        Suppose $T\in\ope{V}$ and $v_1,\dots,v_n$ is a basis of $V$. Then the following are equivalent.
        \begin{enumerate}[label={\textup{(}\alph*\textup{)}}]
            \item The matrix of $T$ with respect to $v_1,\dots,v_n$ is upper triangular.
            \item $Tv_j\in\spn(v_1,\dots,v_j)$ for each $j=1,\dots,n$.
            \item $\spn(v_1,\dots,v_j)$ is invariant under $T$ for each $j=1,\dots,n$.
        \end{enumerate}
        \begin{proof}
            We will first prove that (a) implies (b); the proof in the reverse direction will be symmetric. We will then prove that (b) implies (c), and wrap up by quickly showing that (c) implies (b), and. Let's begin.\par
            Suppose first that the matrix of $T$ with respect to $v_1,\dots,v_n$ is upper triangular. Let $j\in\{1,\dots,n\}$ be arbitrary. By the definition of $\mat{T}$, $Tv_j=A_{1,j}v_1+\cdots+A_{n,j}v_n$. Additionally, since $\mat{T}$ is upper triangular, we know that $A_{j+1,j}=\cdots=A_{n,j}=0$. Therefore, $Tv_j=A_{1,j}v_1+\cdots+A_{j,j}v_j\in\spn(v_1,\dots,v_j)$, as desired. The proof is symmetric in the other direction.\par
            Now suppose that $Tv_j\in\spn(v_1,\dots,v_j)$ for each $j=1,\dots,n$. Fix $j\in\{1,\dots,n\}$. Let $v\in\spn(v_1,\dots,v_j)$. It follows since $Tv_i\in\spn(v_1,\dots,v_i)\subset\spn(v_1,\dots,v_j)$ for all $i=1,\dots,j$ by hypothesis that
            \begin{align*}
                Tv &= T(a_1v_1+\cdots+a_jv_j)\\
                &= a_1Tv_1+\cdots+a_jTv_j\\
                &\in \spn(v_1,\dots,v_j)
            \end{align*}
            as desired. On the other hand, suppose $\spn(v_1,\dots,v_j)$ is invariant under $T$ for each $j=1,\dots,n$. Fix $j\in\{1,\dots,n\}$. Then by the definition of invariance, $v_j\in\spn(v_1,\dots,v_j)$ implies that $Tv_j\in\spn(v_1,\dots,v_j)$, as desired.
        \end{proof}
    \end{theorem}
    \item We now prove that over $\C$ (notably not over $\R$), every operator has an upper-triangular matrix.
    \begin{theorem}
        Suppose $V$ is a finite-dimensional complex vector space and $T\in\ope{V}$. Then $T$ has an upper-triangular matrix with respect to some basis of $V$.\footnote{Note that \textcite{bib:Axler} gives two proofs, but only one (the shorter one using more advanced topics) is transcribed here.}
        \begin{proof}
            We induct on $n=\dim V$. For the base case $n=1$, the desired result holds since any one-dimensional matrix is upper-triangular by definition. Now suppose inductively that we have proven the result for $n-1$; we wish to prove it for $n$. By Theorem \ref{trm:eigenExists}, we may let $v_1$ be an eigenvector of $T$. Thus, if we let $U=\spn(v_1)$, we know that $U$ is an invariant subspace of $T$ with $\dim U=1$.\par
            Now consider $V/U$. It follows from the above by Theroem \ref{trm:dimQuotientSpace} that $\dim V/U=n-1$. Consequently, we may apply the inductive hypothesis to learn that $T/U\in\ope{V/U}$ has an upper-triangular matrix with respect to some basis $v_2+U,\dots,v_n+U$ of $V/U$. It follows by \ref{trm:upperTriangularConditions} that
            \begin{equation*}
                (T/U)(v_j+U) \in \spn(v_2+U,\dots,v_j+U)
            \end{equation*}
            for each $j=2,\dots,n$. Thus, since $Tv_1=\lambda v_1\in\spn(v_1)$ and since the above implies that $Tv_j\in\spn(v_2,\dots,v_j)\subset\spn(v_1,\dots,v_j)$ for all $j=2,\dots,n$, we have that $Tv_j\in\spn(v_1,\dots,v_j)$ for all $j=1,\dots,n$. Therefore, by Theorem \ref{trm:upperTriangularConditions}, we have that the matrix of $T$ with respect to $v_1,\dots,v_n$ is upper triangular, as desired.
        \end{proof}
    \end{theorem}
    \item We now prove an easy method for determining invertibility from an upper-triangular matrix.
    \begin{theorem}\label{trm:upperTriangularInvertible}
        Suppose $T\in\ope{V}$ has an upper-triangular matrix with respect to some basis of $V$. Then $T$ is invertible if and only if all the entries on the diagonal of that upper-triangular matrix are nonzero.
        \begin{proof}
            Let $v_1,\dots,v_n$ be the basis of $V$ with respect to which $\mat{T}$ is upper triangular, and let $\lambda_1,\dots,\lambda_n$ be the diagonal entries of said matrix.\par
            Suppose first that $T$ is invertible, and suppose for the sake of contradiction that $\lambda_j=0$ for some $j=1,\dots,n$. Then by the definition of $\mat{T}$, we have that $T$ maps $\spn(v_1,\dots,v_j)$ into $\spn(v_1,\dots,v_{j-1})$. Thus, since $\dim\spn(v_1,\dots,v_j)=j$ and $\dim\spn(v_1,\dots,v_{j-1})=j-1$, Theorem \ref{trm:smallerNotInjective} implies that the restriction of $T$ to $\spn(v_1,\dots,v_j)$ is not injective. Consequently, by Theorem \ref{trm:nullSpaceInjective}, there exists a nonzero $v\in\spn(v_1,\dots,v_j)$ such that $Tv=0$. But this implies that the original $T$ is not injective, i.e., not invertible (by Theorem \ref{trm:invertibleInjectiveSurjective}), contradicting our supposition that $T$ is invertible, as desired.\par
            Now suppose that $\lambda_j\neq 0$ for all $j=1,\dots,n$. To prove that $T$ is invertible, Theorem \ref{trm:invertInjSurjFiniteEquivalence} tells us that it will suffice to show that $T$ is surjective. To verify that $\range T=V$, we will demonstrate that $v_1,\dots,v_n\in\range T$, from which it will follow by Theorem \ref{trm:rangeSpace} that $\range T=V$. Let's begin. Since $\mat{T}$ is upper triangular, we have that
            \begin{align*}
                Tv_1 &= \lambda_1v_1+0v_2+\cdots+0v_n\\
                v_1 &= T(v_1/\lambda_1) \in \range T
            \end{align*}
            Similarly, we have that
            \begin{align*}
                Tv_2 &= av_1+\lambda_2v_2+0v_3+\cdots+0v_n\\
                v_2 &= T(v_2/\lambda_2)-\frac{a}{\lambda_2}v_1 \in \range T
            \end{align*}
            since $T(v_2/\lambda_2)$ clearly and linear combinations of $v_1$ are elements of $\range T$ by the previous result. We may analogously prove that $v_3,\dots,v_n\in\range T$, as desired.
        \end{proof}
    \end{theorem}
    \item We cannot exactly compute the eigenvalues of a linear operator from its matrix, but we can approximate them with powerful numerical methods.
    \item Lastly, we will show that we can use upper-triangular matrices to determine the eigenvalues of the linear operator it represents.
    \begin{theorem}
        Suppose $T\in\ope{V}$ has an upper-triangular matrix with respect to some basis of $V$. Then the eigenvalues of $T$ are precisely the entries on the diagonal of that upper-triangular matrix.
        \begin{proof}
            Let $v_1,\dots,v_n$ be the basis of $V$ with respect to which $\mat{T}$ is upper triangular, and let $\lambda_1,\dots,\lambda_n$ be the diagonal entries of said matrix.\par
            Let $\lambda\in\F$. Then $\mat{T-\lambda I}$ has diagonal entries $\lambda_1-\lambda,\dots,\lambda_n-\lambda$. Thus, by Theorem \ref{trm:upperTriangularInvertible}, $T-\lambda I$ is not invertible iff $\lambda=\lambda_j$ for some $j=1,\dots,n$. But by Theorem \ref{trm:eigenConditions}, it follows that $\lambda$ is an eigenvalue of $T$ iff $T-\lambda I$ is not invertible iff $\lambda=\lambda_j$ for some $j=1,\dots,n$, as desired.
        \end{proof}
    \end{theorem}
\end{itemize}



\section{Eigenspaces and Diagonal Matrices}
\begin{itemize}
    \item \textbf{Diagonal matrix}: A square matrix that is 0 everywhere except possibly along the diagonal.
    \begin{itemize}
        \item Naturally, every diagonal matrix is upper triangular.
    \end{itemize}
    \item \textbf{Eigenspace} (of $T\in\ope{V}$ and $\lambda\in\F$): The set of all eigenvectors of $T$ corresponding to $\lambda$, along with the zero vector; in other words, the vector space $\nul(T-\lambda I)$. \emph{Denoted by} $\bm{E(\lambda,T)}$.
    \item We now show that the sum of the eigenspaces is a direct sum.
    \begin{theorem}\label{trm:directSumEigenspaces}
        Suppose $V$ is finite-dimensional and $T\in\ope{V}$. Suppose also that $\lambda_1,\dots,\lambda_m$ are distinct eigenvalues of $T$. Then $E(\lambda_1,T)+\cdots+E(\lambda_m,T)$ is a direct sum. Furthermore, $\dim E(\lambda_1,T)+\cdots+\dim E(\lambda_m,T)\leq\dim V$.
        \begin{proof}
            To prove that $E(\lambda_1,T)+\cdots+E(\lambda_m,T)$ is a direct sum, suppose that $u_1+\cdots+u_m=0$, where $u_j\in E(\lambda_j,T)$ for all $j=1,\dots,m$. Naturally, each $u_j$ is an eigenvector corresponding to $\lambda_j$ or zero. But since Theorem \ref{trm:lnlIndependentEigenvectors} implies that $u_1,\dots,u_j$ is linearly independent if any $u_j$ is nonzero, we must have $u_1=\cdots=u_m=0$.\par
            Additionally, we have that
            \begin{align*}
                \dim E(\lambda_1,T)+\cdots+\dim E(\lambda_m,T) &= \dim(E(\lambda_1,T)\oplus\cdots\oplus E(\lambda_m,T))\\
                &\leq \dim V
            \end{align*}
            as desired.
        \end{proof}
    \end{theorem}
    \item \textbf{Diagonalizable} ($T\in\ope{V}$): An operator $T\in\ope{V}$ such that the operator has a diagonal matrix with respect to some basis of $V$.
    \item We now give some conditions equivalent to diagonalizability.
    \begin{theorem}\label{trm:diagonalizableConditions}
        Suppose $V$ is finite-dimensional and $T\in\ope{V}$. Let $\lambda_1,\dots,\lambda_m$ denote the distinct eigenvalues of $T$. Then the following are equivalent.
        \begin{enumerate}[label={\textup{(}\alph*\textup{)}}]
            \item $T$ is diagonalizable.
            \item $V$ has a basis consisting of eigenvectors of $T$.
            \item There exist one-dimensional subspaces $U_1,\dots,U_n$ of $V$, each invariant under $T$, such that $V=U_1\oplus\cdots\oplus U_n$.
            \item $V=E(\lambda_1,T)\oplus\cdots\oplus E(\lambda_m,T)$.
            \item $\dim V=\dim E(\lambda_1,T)+\cdots+\dim E(\lambda_m,T)$.
        \end{enumerate}
        \begin{proof}
            We will first briefly justify the equivalence of (a) and (b). We will then show that (b) implies (c) and vice versa. Lastly, we will show that (b) implies (d), (d) implies (e), and (e) implies (b). Let's begin.\par\smallskip
            Suppose that $T$ is diagonalizable. Then there exists a basis $v_1,\dots,v_n$ of $V$ such that the matrix of $T$ with respect to this basis is diagonal. Let $\lambda_1,\dots,\lambda_n$ be the diagonal entries of this matrix. Then by the definition of $\mat{T}$, $Tv_j=\lambda_jv_j$ for all $j=1,\dots,n$. It follows that each $v_j$ in the basis $v_1,\dots,v_n$ of $V$ is an eigenvector of $T$, as desired. The proof is symmetric in the reverse direction.\par\smallskip
            Suppose that $v_1,\dots,v_n$ is a basis of $V$ consisting of eigenvectors of $T$. Let $U_j=\spn(v_j)$ for each $j=1,\dots,n$. Then each $U_j$ is a one-dimensional subspace of $V$. Additionally, for any $av_j\in U_j$, we have that $T(av_j)=aTv_j=a\lambda_jv_j\in U_j$, proving that each $U_j$ is invariant under $T$. Finally, to show that $V=U_1\oplus\cdots\oplus U_n$, it will suffice to show that each $v\in V$ can be written uniquely as a sum $u_1+\cdots+u_n$, where $u_j\in U_j$ for each $j=1,\dots,n$. Let $v\in V$ be arbitrary. Since $v_1,\dots,v_n$ is a basis of $V$, Theorem \ref{trm:basisLinearCombination} $v$ can be written uniquely in the form $a_1v_1+\cdots+a_nv_n$. Let $u_j=a_jv_j$ for each $j=1,\dots,n$. Then $v$ can be written uniquely as a sum $u_1+\cdots+u_n$, where $u_j\in U_j$ for each $j=1,\dots,n$, as desired. The proof in the reverse direction is quite similar.\par\smallskip
            Suppose that $v_1,\dots,v_n$ is a basis of $V$ consisting of eigenvectors of $T$. Thus, since each $v\in V$ is a linear combination of eigenvectors of $T$, $V=E(\lambda_1,T)+\cdots+E(\lambda_m,T)$. Therefore, Theorem \ref{trm:directSumEigenspaces} implies that $V=E(\lambda_1,T)\oplus\cdots\oplus E(\lambda_m,T)$, as desired.\par
            Suppose $V=E(\lambda_1,T)\oplus\cdots\oplus E(\lambda_m,T)$. Then naturally $\dim V=\dim E(\lambda_1,T)+\cdots+\dim E(\lambda_m,T)$, as desired.\par
            Suppose $\dim V=\dim E(\lambda_1,T)+\cdots+\dim E(\lambda_m,T)$. Choose a basis of each $E(\lambda_j,T)$. Concatenate all these bases to form a list $v_1,\dots,v_n$ of eigenvectors of $T$. We now wish to show that $v_1,\dots,v_n$ is a basis of $V$. To do so, already knowing that $n=\dim V$ by hypothesis, Theorem \ref{trm:sameDimIndependent} tells us that we need only additionally show that $v_1,\dots,v_n$ is linearly independent. Let $a_1,\dots,a_n\in\F$ be such that $a_1v_1+\cdots+a_nv_n=0$. For each $j=1,\dots,m$, let $u_j$ denote the sum of all the terms $a_kv_k$ such that $v_k\in E(\lambda_j,T)$. Thus, each $u_j\in E(\lambda_j,T)$ and $u_1+\cdots+u_m=0$. But since $u_1,\dots,u_m$ are eigenvectors of $T$ corresponding to distinct eigenvalues, Theorem \ref{trm:lnlIndependentEigenvectors} implies that $u_1,\dots,u_m$ are linearly independent. This combined with the previous result implies that $u_j=0$ for each $j=1,\dots,m$. It follows since each (zero) $u_j$ is a linear combination of a basis of an eigenspace, each $a_j=0$, as desired.
        \end{proof}
    \end{theorem}
    \item Note that not every operator is diagonalizable.
    \item We now prove that if an operator has sufficiently many eigenvalues, it is diagonalizable.
    \begin{theorem}\label{trm:diagonalizableEnoughEigenvalues}
        If $T\in\ope{V}$ has $\dim V$ distinct eigenvalues, then $T$ is diagonalizable.
        \begin{proof}
            Let $m=\dim V$, and let $\lambda_1,\dots,\lambda_m$ be the distinct eigenvalues of $T$. Let $v_1,\dots,v_m$ be the corresponding eigenvectors. Since $v_1,\dots,v_m$ has length equal to the dimension of $V$ and is linearly independent (by Theorem \ref{trm:lnlIndependentEigenvectors}), Theorem \ref{trm:sameDimIndependent} implies that $v_1,\dots,v_m$ is a basis of $V$. Therefore, by Theorem \ref{trm:diagonalizableConditions}, $T$ is diagonalizable.
        \end{proof}
    \end{theorem}
    \item Note that the converse of Theorem \ref{trm:diagonalizableEnoughEigenvalues} is not true.
\end{itemize}




\end{document}