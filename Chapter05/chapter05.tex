\documentclass[../main.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter: #1}{}}
\setcounter{chapter}{4}

\begin{document}




\chapter{Eigenvalues, Eigenvectors, and Invariant Subspaces}
\section{Invariant Subspaces}
\begin{itemize}
    \item \marginnote{9/8:}Let $T\in\ope{V}$, and let $V$ be decomposable into a direct sum of proper subspaces as follows.
    \begin{equation*}
        V = U_1\oplus\cdots\oplus U_m
    \end{equation*}
    \begin{itemize}
        \item To understand $T$, we need only understand each each \textbf{restriction} of $T$ to a $U_j$.
        \item Since $T|_{U_j}$ may not map $U_j$ onto itself in every case, to use operator-based tools, we need to consider only direct sum decompositions into subspaces that $T$ maps onto themselves, or \textbf{invariant subspace}.
    \end{itemize}
    \item \textbf{Invariant subspace} (of $V$ under $T$): A subspace $U$ of $V$ such that $u\in U$ implies $Tu\in U$, where $T\in\ope{V}$.
    \begin{itemize}
        \item In other words, $U$ is invariant under $T$ iff $T|_U\in\ope{U}$.
    \end{itemize}
    \item Some invariant subspaces under $T\in\ope{V}$: $\{0\}$, $V$, $\text{null }T$, and $\text{range }T$.
    \item \textbf{Invariant subspace problem}: The most famous unsolved problem in functional analysis, dealing with invariant subspaces of operators on infinite-dimensional vector spaces.
    \item To begin our study of invariant subspaces, we consider the simplest possible type of invariant subspace: those with dimension 1.
    \item Every 1-dimensional subspace of $V$ is of the form $\spn(v)$ for some $v\in V$.
    \begin{itemize}
        \item If $\spn(v)$ is invariant under $T\in\ope{V}$, then $Tv\in\spn(v)$.
        \item If $Tv\in\spn(v)$, then there exists $\lambda\in\F$ such that $Tv=\lambda v$.
    \end{itemize}
    \item \textbf{Eigenvalue} (of $T$): A number $\lambda\in\F$ such that there exists a nonzero vector $v\in V$ satisfying the equation $Tv=\lambda v$. \emph{Also known as} \textbf{characteristic value}.
    \item \dq{$T$ has a 1-dimensional invariant subspace if and only if $T$ has an eigenvalue}{134}
    \item We now give some conditions $\lambda$ can satisfy to be deemed an eigenvalue.
    \begin{theorem}\label{trm:eigenConditions}
        Suppose $V$ is finite-dimensional, $T\in\ope{V}$, $I\in\ope{V}$ is the identity operator on $V$, and $\lambda\in\F$. Then the following are equivalent.
        \begin{enumerate}[label={\textup{(}\alph*\textup{)}}]
            \item $\lambda$ is an eigenvalue of $T$.
            \item $T-\lambda I$ is not injective.
            \item $T-\lambda I$ is not surjective.
            \item $T-\lambda I$ is not invertible.
        \end{enumerate}
        \begin{proof}
            Suppose first that $\lambda$ is an eigenvalue of $T$. Then
            \begin{align*}
                Tv &= \lambda v\\
                Tv &= \lambda Iv\\
                Tv-\lambda Iv &= 0\\
                (T-\lambda I)v &= 0
            \end{align*}
            for some $v\in V$ such that $v\neq 0$. It follows that $v\in\nul(T-\lambda I)$, so by Theorem \ref{trm:nullSpaceInjective}, $T-\lambda I$ is not injective, as desired. The proof is symmetric in the other direction. Therefore, conditions (a) and (b) are equivalent.\par
            To prove that (a), (b), (c), and (d) are equivalent at this point, it will suffice to show that (b), (c), and (d) are equivalent. But we have this by Theorem \ref{trm:invertibleInjectiveSurjective}, as desired.
        \end{proof}
    \end{theorem}
    \item \textbf{Eigenvector} (of $T$): A nonzero vector $v\in V$ such that there exists a $\lambda\in\F$ satisfying the equation $Tv=\lambda v$.
    \item Since $Tv=\lambda v$ iff $(T-\lambda I)v=0$, \dq{a vector $v\in V$ with $v\neq 0$ is an eigenvector of $T$ corresponding to $\lambda$ if and only if $v\in\text{null}(T-\lambda I)$}{135}
    \item Eigenvectors corresponding to distinct eigenvalues are linearly independent.
    \begin{theorem}\label{trm:lnlIndependentEigenvectors}
        Let $T\in\ope{V}$. Suppose $\lambda_1,\dots,\lambda_m$ are distinct eigenvalues of $T$ and $v_1,\dots,v_m$ are corresponding eigenvectors. Then $v_1,\dots,v_m$ is linearly independent.
        \begin{proof}
            Suppose for the sake of contradiction that $v_1,\dots,v_m$ is linearly dependent. Then by the \hyperref[lem:linearDependenceLemma]{Linear Dependence Lemma}, we may let $k$ be the smallest positive integer such that $v_k\in\spn(v_1,\dots,v_{k-1})$. It follows that
            \begin{equation*}
                v_k = a_1v_1+\cdots+a_{k-1}v_{k-1}
            \end{equation*}
            for some $a_1,\dots,a_{k-1}\in\F$. Thus, applying $T$, we have that
            \begin{align*}
                Tv_k &= a_1Tv_1+\cdots+a_{k-1}Tv_{k-1}\\
                \lambda_kv_k &= a_1\lambda_1v_1+\cdots+a_{k-1}\lambda_{k-1}v_{k-1}
            \end{align*}
            If we multiply the first equation by $\lambda_k$ and subtract the above equation from it, we get that
            \begin{equation*}
                0 = a_1(\lambda_k-\lambda_1)v_1+\cdots+a_{k-1}(\lambda_k-\lambda_{k-1})v_{k-1}
            \end{equation*}
            But since $k$ is the smallest positive integer $j$ such that $v_j\in\spn(v_1,\dots,v_{j-1})$, we know that $v_1,\dots,v_{k-1}$ are linearly independent. Thus, $a_1(\lambda_k-\lambda_1)=\cdots=a_{k-1}(\lambda_k-\lambda_{k-1})=0$. More specifically, since all eigenvalues are distinct (i.e., $\lambda_k-\lambda_j\neq 0$ for any $j=1,\dots,k-1$), we must have that $a_1=\cdots=a_{k-1}=0$. But this implies that
            \begin{align*}
                v_k &= a_1v_1+\cdots+a_{k-1}v_{k-1}\\
                &= 0
            \end{align*}
            contradicting the fact that $v_k$, as an eigenvector, is nonzero.
        \end{proof}
    \end{theorem}
    \item We now put a bound on the number of eigenvalues.
    \begin{theorem}
        Suppose $V$ is finite-dimensional. Then each operator on $V$ has at most $\dim V$ distinct eigenvalues.
        \begin{proof}
            Let $T\in\ope{V}$ have distinct eigenvalues $\lambda_1,\dots,\lambda_m$ and corresponding eigenvectors $v_1,\dots,v_m$. Then by Theorem \ref{trm:lnlIndependentEigenvectors}, $v_1,\dots,v_m$ is linearly independent. It follows by Theorem \ref{trm:linearIndependent-Spanning} that $m\leq\dim V$
        \end{proof}
    \end{theorem}
    \item \textbf{Restriction operator} (of $T:V\to W$ to $U\subset V$): The function $T|_U:U\to W$ defined by $T|_U(u)=Tu$ for all $u\in U$. \emph{Denoted by} $\bm{T|_U}$.
    \begin{itemize}
        \item The fact that $U$ is invariant under $T$ is what allows us to consider $T|_U$ to be in $\ope{U}$ as opposed to just $\ope{V}$.
    \end{itemize}
    \item \textbf{Quotient operator}: The operator $T/U\in\ope{V/U}$ defined by $(T/U)(v+U)=Tv+U$ for all $v\in V$.
    \item \textcite{bib:Axler} verifies that the restriction operator and the quotient operator actually \emph{are} operators, in general, as defined.
\end{itemize}



\section{Eigenvectors and Upper-Triangular Matrices}
\begin{itemize}
    \item \marginnote{9/10:}If an operator $T\in\ope{V}$, then $TT=T^2\in\ope{V}$.
    \item $\bm{T^m}$: The operator $T^m\in\ope{V}$ defined by
    \begin{equation*}
        T^m = \underbrace{T\cdots T}_\text{$m$ times}
    \end{equation*}
    where $T\in\ope{V}$, $m\in\N$.
    \item $\bm{T^0}$: The identity operator $I\in\ope{V}$, where $T\in\ope{V}$.
    \item $\bm{T^{-m}}$: The operator $T^{-m}\in\ope{V}$ defined by
    \begin{equation*}
        T^{-m} = (T^{-1})^m
    \end{equation*}
    where $T\in\ope{V}$ is invertible with inverse $T^{-1}$, and $m\in\N$.
    \item It follows from these definitions that
    \begin{align*}
        T^mT^n &= T^{m+n}&
        (T^m)^n &= T^{mn}
    \end{align*}
    for any $m,n\in\Z$ if $T$ is invertible and for any $m,n\in\N$ if $T$ is not invertible.
    \item $\bm{p(T)}$: The operator defined by
    \begin{equation*}
        p(T) = a_0I+a_1T+\cdots+a_mT^m
    \end{equation*}
    where $T\in\ope{V}$, and $p\in\pol{\F}$ is defined by $p(z) = a_0+a_1z+\cdots+a_mz^m$ for all $z\in\F$.
    \item $f:\pol{\F}\to\ope{V}$ defined by $p\mapsto p(T)$ is linear.
    \item \textbf{Product} (of $p,q\in\pol{\F}$): The polynomial $pq\in\pol{\F}$ defined by $(pq)(z)=p(z)q(z)$ for all $z\in\F$.
    \item Multiplicative properties of $p(T)$.
    \begin{theorem}\label{trm:polTMultiplicative}
        Suppose $p,q\in\pol{\F}$ and $T\in\ope{V}$. Then
        \begin{enumerate}[label={\textup{(}\alph*\textup{)}}]
            \item $(pq)(T)=p(T)q(T)$.
            \begin{proof}
                Suppose $p(z)=\sum_{j=0}^ma_jz^j$ and $q(z)=\sum_{k=0}^nb_kz^k$ for all $z\in\F$. Then
                \begin{equation*}
                    (pq)(z) = \sum_{j=0}^m\sum_{k=0}^na_jb_kz^{j+k}
                \end{equation*}
                so
                \begin{align*}
                    (pq)(T) &= \sum_{j=0}^m\sum_{k=0}^na_jb_kT^{j+k}\\
                    &= \left( \sum_{j=0}^ma_jT^j \right)\left( \sum_{k=0}^nb_kT^k \right)\\
                    &= p(T)q(T)
                \end{align*}
                as desired.
            \end{proof}
            \item $p(T)q(T)=q(T)p(T)$.
            \begin{proof}
                It follows from part (a) that $p(T)q(T)=(pq)(T)=(qp)(T)=q(T)p(T)$, as desired.
            \end{proof}
        \end{enumerate}
    \end{theorem}
    \item We now prove a central result concerning operators on complex vector spaces.
    \begin{theorem}\label{trm:eigenExists}
        Every operator on a finite-dimensional, nonzero, complex vector space has an eigenvalue.
        \begin{proof}
            Let $V$ be nonzero complex vector space of dimension $n$, let $T\in\ope{V}$, and let $v\in V$ be nonzero. Then since $\len(v,Tv,T^2v,\dots,T^nv)>\dim V$, Theorem \ref{trm:linearIndependent-Spanning} implies that $v,Tv,\dots,T^nv$ is not linearly independent. Thus, there exist $a_0,\dots,a_n\in\F$, not all 0, such that
            \begin{equation*}
                0 = a_0v+a_1Tv+\cdots+a_nT^nv
            \end{equation*}
            Additionally, we have that $a_1,\dots,a_n$ do not all equal zero (suppose they did; then $0=a_0v$, so $a_0=0$ because $v\neq 0$; this would imply that $a_0=\cdots=a_n=0$, a contradiction). Thus, if we consider the (nonconstant, by the previous result) polynomial with coefficients equal to the $a$'s, Theorem \ref{trm:polFactorization} implies that
            \begin{equation*}
                a_0+a_1z+\cdots+a_nz^n = c(z-\lambda_1)\cdots(z-\lambda_m)
            \end{equation*}
            for all $z\in\C$, where $c\in\C$ is nonzero and each $\lambda_j\in\C$\footnote{Note that $m\neq n$ necessarily, because $a_n$ may equal 0.}. It follows that
            \begin{align*}
                0 &= a_0v+a_1Tv+\cdots+a_nT^nv\\
                &= (a_0I+a_1T+\cdots+a_nT^n)v\\
                &= c(T-\lambda_1I)\cdots(T-\lambda_mI)v\tag*{Theorem \ref{trm:polTMultiplicative}}
            \end{align*}
            Each $T-\lambda_jI$ is a linear operator in its own right. As a linear map, we have by Theorem \ref{trm:linSendsZero} that $(T-\lambda_jI)0=0$ for all $j=1,\dots,m$. However, $v\neq 0$, meaning that there exists $1\leq k\leq m$ such that $T-\lambda_kI$ maps its nonzero argument, which we may call $v'$ and know to be the result of all of the operators to its right being applied successively to $v$ via function composition, to 0 as well; every operator to the left of $T-\lambda_kI$ will then map the result to 0, resulting in the final equivalence. Thus, $(T-\lambda_kI)v'=(T-\lambda_kI)0$ but $v'\neq 0$ for this operator, meaning that it is not injective. But if $T-\lambda_kI$ is not injective, Theorem \ref{trm:eigenConditions} implies that $\lambda_k$ is an eigenvalue of $T$, as desired.
        \end{proof}
    \end{theorem}
    \item \textbf{Matrix} (of $T\in\ope{V}$ with respect to the basis $v_1,\dots,v_n$ of $V$): The $n$-by-$n$ matrix $\mat{T}$ whose entries $A_{j,k}$ are defined by
    \begin{equation*}
        Tv_k = A_{1,k}v_1+\cdots+A_{n,k}v_n
    \end{equation*}
    \begin{itemize}
        \item If the basis is not clear from context, then the notation $\mat{T,(v_1,\dots,v_n)}$ is used.
    \end{itemize}
    \item \dq{A central goal of linear algebra is to show that given an operator $T\in\ope{V}$, there exists a basis of $V$ with respect to which $T$ has a reasonably simple [sparse] matrix}{146}
    \begin{itemize}
        \item For example, given just Theorems \ref{trm:columnBasis} and \ref{trm:eigenExists}, we know that we can choose a basis of $V$ such that the first column of $\mat{T}$ for an arbitrary $T$ will have an eigenvalue $\lambda$ of $T$ in the first row and zeros everywhere else. Specifically, let $v_1$ be the eigenvector corresponding to $\lambda$ (guaranteed to exist by Theorem \ref{trm:eigenExists}). Then by Theorem \ref{trm:columnBasis},
        \begin{align*}
            \mat{T}_{\cdot,1} &= \mat{Tv_1}\\
            &= \mat{\lambda v_1}\\
            &= \lambda\mat{v_1}+0\mat{v_2}+\cdots+0\mat{v_n}
        \end{align*}
        where we extend $v_1$ into a basis $v_1,\dots,v_n$ of $V$.
    \end{itemize}
    \item \textbf{Diagonal} (of a square matrix): The entries along the line from the upper left corner to the bottom right corner.
    \item \textbf{Upper-triangular matrix}: A matrix such that all the entries below the diagonal equal 0. \emph{Denoted by}
    \begin{equation*}
        \begin{pmatrix}
            \lambda_1 &  & *\\
             & \ddots & \\
            0 &  & \lambda_n\\
        \end{pmatrix}\footnotemark
    \end{equation*}
    \footnotetext{We often use $*$ to denote matrix entries that we do not know about or that are irrelevant to the questions being discussed.}
\end{itemize}




\end{document}